{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededd5fa-4ea5-4055-beb5-2ead0f1cbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Clustering\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1d6b6-ecaa-40aa-af5e-9952e137fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363959e-b31e-4325-9f9f-8d60ccbc4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import kneed\n",
    "import itertools\n",
    "import shapely\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from CGAL.CGAL_Alpha_shape_2 import *\n",
    "from CGAL.CGAL_Kernel import Point_2\n",
    "from sklearn.cluster import DBSCAN, OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import LineString\n",
    "from shapely.ops import polygonize, cascaded_union\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from shapely.ops import polygonize_full, linemerge, unary_union\n",
    "from scipy.spatial import cKDTree, Delaunay\n",
    "from graph_tool.all import triangulation, label_components\n",
    "from scipy.linalg import norm\n",
    "\n",
    "import hdbscan\n",
    "import graph_tool\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from HierarchicalGeoClustering.TreeClusters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050786f1-c002-4a84-ae0f-429e6bd7c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def get_alpha_shape(point_list):\n",
    "    \"\"\"\n",
    "    Returns a polygon representing the hull of the points sample.\n",
    "    \n",
    "    :param list point_list: list list of tuples with samples coordinates.\n",
    "    \n",
    "    :returns shapely.Polygon: concave hull shapely polygon\n",
    "    \"\"\"\n",
    "    uni_po = np.unique(point_list, axis=0)\n",
    "    if len(uni_po) < 3:\n",
    "        raise ValueError('Alpha Shape needs more than 3 points')\n",
    "    if set_colinear(uni_po) == True:\n",
    "        raise ValueError('The set of points can be colinear')\n",
    "\n",
    "    list_of_points = [Point_2(l[0], l[1]) for l in point_list]\n",
    "\n",
    "    a = Alpha_shape_2()\n",
    "\n",
    "    a.make_alpha_shape(list_of_points)\n",
    "    a.set_mode(REGULARIZED)\n",
    "    alpha = a.find_optimal_alpha(1).next()\n",
    "    a.set_alpha(alpha)\n",
    "\n",
    "    edges = []\n",
    "    for it in a.alpha_shape_edges():\n",
    "        edges.append(a.segment(it))\n",
    "\n",
    "    lines = []\n",
    "    for e in edges:\n",
    "        source_p = (e.source().x(), e.source().y())\n",
    "        target_p = (e.target().x(), e.target().y())\n",
    "        lines.append(LineString([source_p, target_p]))\n",
    "\n",
    "    return unary_union(list(polygonize(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a8f35-6709-49ba-9799-1ca8e5f56ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def set_colinear(list_points):\n",
    "    \"\"\"\n",
    "    Check if in the list of points any of triplet of points\n",
    "    is colinear\n",
    "    :param list list_points: List of shapely Points\n",
    "    \n",
    "    :returns bool: True if all are not colinear \n",
    "    \"\"\"\n",
    "    for i in itertools.combinations(list_points, 3):\n",
    "        if collinear(i[0], i[1], i[2]) == False:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee172fb9-626f-4d50-850f-def98bc73bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def collinear(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    Check if the points are colinear \n",
    "    \n",
    "    :param shapely Point p1: point to chek if is colinear\n",
    "    \n",
    "    :param shapely Point p2: point to chek if is colinear\n",
    "    \n",
    "    :param shapely Point p3: point to chek if is colinear\n",
    "    \n",
    "    :return bool: True if are colinear\n",
    "    \"\"\"\n",
    "    return (p1[1]-p2[1]) * (p1[0]-p3[0]) == (p1[1]-p3[1])*(p1[0]-p2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34f107-39d2-4e5c-a460-f214a09eba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-0.031204001606285635 0.030660687427946844 1.0262842316887542 0.9798197476399663\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,1.04114112249586)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.020525684633775083\" opacity=\"0.6\" d=\"M 0.38952305204905313,0.7340942216547534 L 0.41420115255470136,0.5385894588374259 L 0.6614807172483953,0.45412205733378397 L 0.8101970929488229,0.6402325117874904 L 0.9450833190355996,0.3771682961289605 L 0.9570697029828851,0.10776268381461007 L 0.5749227293925475,0.23188438677968415 L 0.47148129100495406,0.14556106255425982 L 0.07831021882139266,0.06867121452753033 L 0.006806525493297855,0.0793162252522136 L 0.019550650340822373,0.3799281660378486 L 0.08122466992727406,0.7072800254555981 L 0.08778416260412392,0.9724699079683295 L 0.1370390579819729,0.8881804888641214 L 0.36394499519237145,0.8286302244374212 L 0.7607358775941138,0.9127497016317572 L 0.38952305204905313,0.7340942216547534 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((0.39 0.734, 0.414 0.539, 0.661 0.454, 0.81 0.64, 0.945 0.377, 0.9...>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "x = [random.random() for i in range(20)] \n",
    "y = [random.random() for i in range(20)] \n",
    "points_check = [i for i in zip(x,y)]\n",
    "get_alpha_shape(points_check) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730387c7-9988-4be2-a3cc-f160e8169352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "### bouth are checked\n",
    "set_colinear(points_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2bcd9-6a58-4cb9-b971-26a87f7db85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def get_segments(points):\n",
    "    \"\"\" \n",
    "    Get the segments from a delaunay triangulation\n",
    "    \n",
    "    :param points: Point to get Delaunay triangulation and exctract points \n",
    "    \n",
    "    :return edges: \n",
    "    \"\"\"\n",
    "    TIN = Delaunay(points)\n",
    "    # list of coordinates for each edge\n",
    "    edges = []\n",
    "    for tr in TIN.simplices:\n",
    "        for i in range(3):\n",
    "            edge_idx0 = tr[i]\n",
    "            edge_idx1 = tr[(i+1) % 3]\n",
    "            edges.append(LineString((Point(TIN.points[edge_idx0]),\n",
    "                                    Point(TIN.points[edge_idx1]))))\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96558b0c-b036-4772-87f8-d5d518a6b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "lines = get_segments(points_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c930c-2913-4b1e-ba9f-d10f41c09caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def get_polygons_buf(lines):\n",
    "    \"\"\"\n",
    "    Obtain the poligons from the lines\n",
    "    \n",
    "    :param list lines: List of lines\n",
    "    \n",
    "    :returns shapely polygon: the union of the union of \n",
    "    edges (Polygon or multypolygon)\n",
    "    \"\"\"\n",
    "    linework = linemerge(lines)\n",
    "    linework = unary_union(linework)\n",
    "    result, _, _, _ = polygonize_full(linework)\n",
    "    result = unary_union(result)\n",
    "    result = result.buffer(0.0000001)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67e95e-3769-49e2-b61a-d2d323654452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-0.031204109509001418 0.030660579534288107 1.0262844474850972 0.9798199635228585\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,1.0411411225914347)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.020525688949701942\" opacity=\"0.6\" d=\"M 0.006806510768224136,0.07931612634229396 L 0.00680650119110973,0.07931612825013294 L 0.0068064918457433465,0.07931613108299318 L 0.006806482821243307,0.07931613481386023 L 0.006806474203668119,0.07931613940715619 L 0.006806466075195814,0.07931614481907895 L 0.0068064585133402935,0.07931615099801988 L 0.006806451590212145,0.079316157885056 L 0.00680644537183099,0.07931616541451188 L 0.006806439917495914,0.07931617351458588 L 0.006806435279219986,0.07931618210803491 L 0.006806431501234255,0.07931619111291102 L 0.006806428619565959,0.07931620044334284 L 0.006806426661694966,0.07931621001035445 L 0.006806425646291725,0.0793162197227139 L 0.006806425583039218,0.0793162294878032 L 0.019550550430563737,0.3799281702734382 L 0.01955055099732186,0.37992817747764346 L 0.08778406326062341,0.9724699194081244 L 0.08778406482591151,0.9724699289307048 L 0.08778406730180482,0.9724699382580634 L 0.08778407066524543,0.9724699473033346 L 0.08778407488490973,0.9724699559822803 L 0.08778407992150013,0.9724699642140736 L 0.08778408572811103,0.9724699719220522 L 0.08778409225066566,0.972469979034432 L 0.08778409942841969,0.9724699854849754 L 0.08778410719452694,0.9724699912136091 L 0.08778411547666189,0.9724699961669822 L 0.08778412419769333,0.9724700002989641 L 0.08778413327640255,0.972470003571074 L 0.08778414262823987,0.9724700059528388 L 0.08778415216611192,0.9724700074220771 L 0.08778416180119283,0.972470007965106 L 0.08778417144375142,0.9724700075768683 L 0.7607358864337412,0.9127498012402959 L 0.7607358959308291,0.9127497999362071 L 0.7607359052594113,0.9127497977287463 L 0.7607359143337622,0.912749794638199 L 0.7607359230704929,0.9127497906929661 L 0.7607359313893169,0.9127497859293023 L 0.7607359392137882,0.9127497803909834 L 0.7607359464720035,0.9127497741289041 L 0.760735953097263,0.9127497672006099 L 0.7607359590286841,0.9127497596697685 L 0.7607359642117594,0.9127497516055849 L 0.7607359685988591,0.9127497430821653 L 0.7607359721496677,0.9127497341778357 L 0.9450834135911536,0.37716832867503897 L 0.9450834162679029,0.37716831949301827 L 0.9450834180552248,0.3771683100972761 L 0.9450834189367696,0.37716830057375944 L 0.9570698028840551,0.10776268825940902 L 0.9570698028386677,0.1077626784459347 L 0.9570698018316058,0.10776266868416413 L 0.9570697998725682,0.10776265906810936 L 0.9570697969804215,0.10776264969037906 L 0.957069793183019,0.10776264064128674 L 0.9570697885169319,0.10776263200798089 L 0.9570697830270978,0.1077626238736057 L 0.9570697767663873,0.10776261631650039 L 0.9570697697950947,0.10776260940944464 L 0.9570697621803583,0.10776260321895781 L 0.9570697539955126,0.10776259780465819 L 0.9570697453193832,0.1077625932186889 L 0.9570697362355263,0.10776258950521578 L 0.9570697268314254,0.10776258670000188 L 0.957069717197648,0.10776258483006322 L 0.9570697074269734,0.10776258391340848 L 0.07831022326548098,0.06867111462632874 L 0.07831021365706065,0.06867111466097099 L 0.07831020409631895,0.06867111561761069 L 0.006806510768224136,0.07931612634229396 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((0.007 0.079, 0.007 0.079, 0.007 0.079, 0.007 0.079, 0.007 0.079, ...>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "get_polygons_buf(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d55338-5e1a-4fa7-a5a9-27356b4b201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def jaccard_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between two polygons.\n",
    "    \n",
    "    param: p1 shapely Poligon \n",
    "    param: p2 shapely Poligon \n",
    "    return float Jaccard distance\n",
    "    \"\"\"\n",
    "    intersection_area = p1.intersection(p2).area  \n",
    "    #print(intersection_area)\n",
    "    jacc= 1 - (intersection_area)/(p1.area + p2.area - intersection_area)\n",
    "    return jacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1615da1-a1e5-410b-a3da-7fb35744bdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6126067634003338"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "x = [random.random() for i in range(20)] \n",
    "y = [random.random() for i in range(20)] \n",
    "x2 = [random.random() for i in range(20)] \n",
    "y2 = [random.random() for i in range(20)] \n",
    "points_check = [i for i in zip(x,y)]\n",
    "points_check2 = [i for i in zip(x2,y2)]\n",
    "\n",
    "poly = get_alpha_shape(points_check) \n",
    "poly2= get_alpha_shape(points_check2)\n",
    "jaccard_distance(poly, poly2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed5002-7b66-42ac-b335-7f52c0b749b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6fb764f-3cf9-4bd3-8495-c4caed8c30aa",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "In this module all the clustering methods wrap or implemented (Natural cities, DBSCAN, OPTICS, HDBSCAN, and adap_DBSCAN) all with the intention to have the same input and output. \n",
    "\n",
    "A recursive function is implemented to obtain the cluster iterative using the output as the new input. To select the method to use only a string is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b9b2a-9ced-4043-87bd-950235e99ed6",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "A wapper functions to obtain the clusterizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646c9cf-05c5-4d02-a3db-030b3834ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_dbscan(cluster,  **kwargs):\n",
    "    \n",
    "    \"\"\" \n",
    "    Sklearn DBSCAN wrapper.\n",
    "    \n",
    "    :param cluster: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :param eps_DBSCAN: Minimal epsilon distance (Default = .04)\n",
    "    \n",
    "    :param debugg: To print Debugg information (Default = False)\n",
    "    \n",
    "    :param min_samples: Minimal salmples DBSCAN (Default= 50)\n",
    "    \n",
    "    :param return_noise: If True the noise is also return  (Default= True)\n",
    "    \n",
    "    :param scale_points: If True scale the data to [1,0]  (Default= True)\n",
    "    \n",
    "    :returns list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    eps = kwargs.get( 'eps_DBSCAN',.04)\n",
    "    debugg= kwargs.get( 'verbose',False)\n",
    "    min_samples= kwargs.get( 'min_samples',50)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    scale_points= kwargs.get('scale_points', True)\n",
    "    # Standarize sample\n",
    "    scaler = StandardScaler()\n",
    "    if scale_points ==True:\n",
    "        if debugg:\n",
    "            print('Scaling the points')\n",
    "        cluster = scaler.fit_transform(cluster)\n",
    "        \n",
    "    \n",
    "    if debugg:\n",
    "        print('epsilon distance to DBSCAN: ', eps)\n",
    "        print(\"min_samples to DBScan: \", min_samples )\n",
    "        print(\"Number of points to fit the DBScan: \",cluster.shape[0])\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(cluster)  # Check if can be run with n_jobs = -1\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels)\n",
    "    ##### returning the point to the original\n",
    "    if scale_points ==True:\n",
    "        if debugg:\n",
    "            print('Return the points to original coordinates')\n",
    "        cluster = scaler.inverse_transform(cluster)\n",
    "    \n",
    "    clusters = []\n",
    "    #######check that not returning the same cluster \n",
    "#     if len(unique_labels) == 1 and len(cluster) == sum(labels == 0):\n",
    "#         if debugg:\n",
    "#             print('Its the same set of points after clustering')\n",
    "#             print('Only one cluster with the same number of points \\n')\n",
    "#             print('Returns the points as noise')\n",
    "        \n",
    "#         if ret_noise == True:\n",
    "#             class_member_mask = (labels == 0)\n",
    "#             return clusters, points_ret[class_member_mask]\n",
    "#         else:\n",
    "#             return clusters # return empty cluster\n",
    "    \n",
    "    ########\n",
    "    if debugg:\n",
    "        print('Number of clusters:' ,l_unique_labels)\n",
    "    \n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(cluster[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "    \n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, cluster[class_member_mask]\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88e40b-6f7a-480a-a346-25146d817c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "### Pruebas \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_DBSCAN_clus, noise = compute_dbscan(X_2, return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a439b-dcb8-45da-9e27-aa9374dca393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### compute_dbscan\n",
       "\n",
       ">      compute_dbscan (cluster, **kwargs)\n",
       "\n",
       "*Sklearn DBSCAN wrapper.\n",
       "\n",
       ":param cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":param eps_DBSCAN: Minimal epsilon distance (Default = .04)\n",
       "\n",
       ":param debugg: To print Debugg information (Default = False)\n",
       "\n",
       ":param min_samples: Minimal salmples DBSCAN (Default= 50)\n",
       "\n",
       ":param return_noise: If True the noise is also return  (Default= True)\n",
       "\n",
       ":param scale_points: If True scale the data to [1,0]  (Default= True)\n",
       "\n",
       ":returns list with numpy arrays for all the clusters obtained*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### compute_dbscan\n",
       "\n",
       ">      compute_dbscan (cluster, **kwargs)\n",
       "\n",
       "*Sklearn DBSCAN wrapper.\n",
       "\n",
       ":param cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":param eps_DBSCAN: Minimal epsilon distance (Default = .04)\n",
       "\n",
       ":param debugg: To print Debugg information (Default = False)\n",
       "\n",
       ":param min_samples: Minimal salmples DBSCAN (Default= 50)\n",
       "\n",
       ":param return_noise: If True the noise is also return  (Default= True)\n",
       "\n",
       ":param scale_points: If True scale the data to [1,0]  (Default= True)\n",
       "\n",
       ":returns list with numpy arrays for all the clusters obtained*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compute_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2e2cc-3ead-46df-913d-81b233979560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adaptative_DBSCAN(points2_clusters ,\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    The function use the knee and average to obtain a good value for epsilon and use \n",
    "    DBSCAN to obtain the clusters\n",
    "    \n",
    "    :param list Points points2_clusters: Point to clusterize  \n",
    "    \n",
    "    :param int max_k: = (Default = len(points2_clusters)*.1)\n",
    "    \n",
    "    :param int  min_k: (Default =50)\n",
    "    \n",
    "    :param int step_k: (Default = 50)\n",
    "    \n",
    "    :param int leaf_size: (Default = 50)\n",
    "    \n",
    "    :param bool scale_points: (Default = True)\n",
    "    \n",
    "    :param bool debugg: (Default = False)\n",
    "    \n",
    "    :param bool ret_noise:  (Default = True)\n",
    "    \n",
    "    :returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise \n",
    "    \"\"\"\n",
    "    max_k = kwargs.get('max_k', int(len(points2_clusters)*.1))\n",
    "    max_k_percent = kwargs.get('max_k_percent', None)\n",
    "    min_k = kwargs.get('min_k', 50)\n",
    "    step_k = kwargs.get('step_k', 50)\n",
    "    leaf_size =  kwargs.get('leaf_size',50)\n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    ###### Se tienen que hacer algunos cambios para cuando\n",
    "    #  los clusters son menores a los minimos establecidos previemente\n",
    "    \n",
    "    ##### Establecer los minimos y maximos posibles \n",
    "    if max_k > len(points2_clusters):\n",
    "        raise ValueError('The max_k value is too large for the number of points')\n",
    "    \n",
    "    if max_k_percent != None:\n",
    "        max_k = int(len(points2_clusters)*max_k_percent)\n",
    "    \n",
    "    if min_k >  len(points2_clusters):\n",
    "        print('The min_k value is too large for the number of points returns empty clusters')\n",
    "        if ret_noise == True:\n",
    "            return [] , points2_clusters\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    if step_k > len(points2_clusters):\n",
    "        raise ValueError('The step_k value is too large for the number of points')\n",
    "\n",
    "    \n",
    "    if min_k == max_k:\n",
    "        print('min_k reset to obtain at least 1 value')\n",
    "        min_k = max_k-1\n",
    "\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "    \n",
    "    kdt=  cKDTree(points_arr, leafsize=leaf_size)\n",
    "    lits_appe_all_aver=[]\n",
    "    for j in range( min_k, max_k, step_k ):\n",
    "        dist_va, ind = kdt.query(points_arr, k=j, workers =-1) \n",
    "        non_zero =  dist_va[:, 1:]\n",
    "        non_zero = np.ndarray.flatten(non_zero)\n",
    "        non_zero = np.sort(non_zero)\n",
    "        lis_aver_k=[]\n",
    "        for i in range(int(non_zero.shape[0]/(j-1)) -1):\n",
    "            lis_aver_k.append(np.average(non_zero[i*(j-1):(i+1)*(j-1)]))\n",
    "\n",
    "        average_arr= np.array(lis_aver_k)\n",
    "        kneedle_1_average = kneed.KneeLocator(\n",
    "                range(average_arr.shape[0]),\n",
    "                average_arr,\n",
    "                curve=\"convex\",## This should be the case since the values are sorted \n",
    "                direction=\"increasing\", ## This should be the case since the values are sorted incresing\n",
    "                online=True, ### To find the correct knee the false returns the first find \n",
    "        )\n",
    "        epsilon= kneedle_1_average.knee_y\n",
    "        min_point = kneedle_1_average.knee\n",
    "        #### We take the average never the less\n",
    "        \n",
    "        lits_appe_all_aver.append({ 'k':j,\n",
    "                    'Epsilon':epsilon,\n",
    "                    'value':min_point})\n",
    "    \n",
    "    #### Check if the list is empty\n",
    "    if len(lits_appe_all_aver) ==0:\n",
    "        if debugg:\n",
    "            print('DBSCAN')\n",
    "            print('Using 0.6 as epsilon and 20 as Minpoints')\n",
    "        db_scan= DBSCAN(eps=0.6, min_samples=20).fit(points_arr)\n",
    "    else:\n",
    "        df_all_average= pd.DataFrame(lits_appe_all_aver)\n",
    "        max_epsi_all_average= df_all_average['Epsilon'].max()\n",
    "        if debugg:\n",
    "            print('Valor de epsion  : ', max_epsi_all_average)\n",
    "        db_scan= DBSCAN(eps=max_epsi_all_average, min_samples=min_k).fit(points_arr)\n",
    "    \n",
    "    ####Get the clusters\n",
    "    core_samples_mask = np.zeros_like(db_scan.labels_, dtype=bool)\n",
    "    core_samples_mask[db_scan.core_sample_indices_] = True\n",
    "    labels = db_scan.labels_\n",
    "    unique_labels = set(labels)\n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "    #######check that not returning the same cluster \n",
    "    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):\n",
    "        if debugg:\n",
    "            print('Only one cluster with the same number of points \\n')\n",
    "            print('Returns the points as noise')\n",
    "        print('Its the same set of points after clustering')\n",
    "        if ret_noise == True:\n",
    "            class_member_mask = (labels == 0)\n",
    "            return clusters, points_ret[class_member_mask]\n",
    "        else:\n",
    "            return clusters # return empty cluster\n",
    "    \n",
    "    ########\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfef8e-165e-446b-84a3-2ebfe602f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "### Test\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_Adaptative_clus, noise = adaptative_DBSCAN(X_2, return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a16d3-40bc-4587-ba39-35a2305fceff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### adaptative_DBSCAN\n",
       "\n",
       ">      adaptative_DBSCAN (points2_clusters, **kwargs)\n",
       "\n",
       "*The function use the knee and average to obtain a good value for epsilon and use \n",
       "DBSCAN to obtain the clusters\n",
       "\n",
       ":param list Points points2_clusters: Point to clusterize  \n",
       "\n",
       ":param int max_k: = (Default = len(points2_clusters)*.1)\n",
       "\n",
       ":param int  min_k: (Default =50)\n",
       "\n",
       ":param int step_k: (Default = 50)\n",
       "\n",
       ":param int leaf_size: (Default = 50)\n",
       "\n",
       ":param bool scale_points: (Default = True)\n",
       "\n",
       ":param bool debugg: (Default = False)\n",
       "\n",
       ":param bool ret_noise:  (Default = True)\n",
       "\n",
       ":returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### adaptative_DBSCAN\n",
       "\n",
       ">      adaptative_DBSCAN (points2_clusters, **kwargs)\n",
       "\n",
       "*The function use the knee and average to obtain a good value for epsilon and use \n",
       "DBSCAN to obtain the clusters\n",
       "\n",
       ":param list Points points2_clusters: Point to clusterize  \n",
       "\n",
       ":param int max_k: = (Default = len(points2_clusters)*.1)\n",
       "\n",
       ":param int  min_k: (Default =50)\n",
       "\n",
       ":param int step_k: (Default = 50)\n",
       "\n",
       ":param int leaf_size: (Default = 50)\n",
       "\n",
       ":param bool scale_points: (Default = True)\n",
       "\n",
       ":param bool debugg: (Default = False)\n",
       "\n",
       ":param bool ret_noise:  (Default = True)\n",
       "\n",
       ":returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(adaptative_DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c167d-92a4-4220-9c06-14e536b6dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_hdbscan(points2_clusters,  **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    HDBSCAN wrapper.\n",
    "    \n",
    "    :param np.array cluster: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :returns:  list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    \n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    min_cluster = kwargs.get('min_cluster', 20)\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "    db = hdbscan.HDBSCAN( ).fit(points_arr)\n",
    "    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels)\n",
    "    if debugg:\n",
    "        print('total number of clusters: ', len(unique_labels)) \n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "    #######check that not returning the same cluster \n",
    "    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):\n",
    "        if debugg:\n",
    "            print('Its the same set of points after clustering')\n",
    "            print('Only one cluster with the same number of points \\n')\n",
    "            print('Returns the points as noise')\n",
    "        \n",
    "        if ret_noise == True:\n",
    "            class_member_mask = (labels == 0)\n",
    "            return clusters, points_ret[class_member_mask]\n",
    "        else:\n",
    "            return clusters # return empty cluster\n",
    "    \n",
    "    ########\n",
    "\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1e4c8-ec0d-43a2-b6ff-4c41bf05751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/miniconda3/envs/HCL/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/miguel/miniconda3/envs/HCL/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "### Test\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_HDBSCAN_clus, noise = compute_hdbscan(X_2, return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8517503-4f13-4b67-9970-3f49c058d540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### compute_hdbscan\n",
       "\n",
       ">      compute_hdbscan (points2_clusters, **kwargs)\n",
       "\n",
       "*HDBSCAN wrapper.\n",
       "\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns:  list with numpy arrays for all the clusters obtained*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### compute_hdbscan\n",
       "\n",
       ">      compute_hdbscan (points2_clusters, **kwargs)\n",
       "\n",
       "*HDBSCAN wrapper.\n",
       "\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns:  list with numpy arrays for all the clusters obtained*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compute_hdbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799aca1-c7a2-48aa-8e1e-bc0d037bcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_OPTICS(points2_clusters,  **kwargs):\n",
    "    \n",
    "    \"\"\" OPTICS wrapper.\n",
    "    :param np.array cluster: a (N,2) numpy array containing the obsevations\n",
    "    :returns:  list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "\n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    min_samples= kwargs.get( 'min_samples',5)\n",
    "    eps_optics = kwargs.get('eps_optics', None)\n",
    "    n_jobs = kwargs.get('num_jobs',None)\n",
    "    xi= kwargs.get('xi',None)\n",
    "    algorithm_optics= kwargs.get('algorithm_optics','kd_tree')\n",
    "\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "\n",
    "    db = OPTICS(min_samples = min_samples,eps= eps_optics, n_jobs= n_jobs).fit(points2_clusters)\n",
    "    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels)\n",
    "    if debugg:\n",
    "        print('total number of clusters: ', len(unique_labels)) \n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "    #######check that not returning the same cluster \n",
    "    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):\n",
    "        if debugg:\n",
    "            print('Its the same set of points after clustering')\n",
    "            print('Only one cluster with the same number of points \\n')\n",
    "            print('Returns the points as noise')\n",
    "        \n",
    "        if ret_noise == True:\n",
    "            class_member_mask = (labels == 0)\n",
    "            return clusters, points_ret[class_member_mask]\n",
    "        else:\n",
    "            return clusters # return empty cluster\n",
    "    \n",
    "    ########\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a18f5-3d42-4b26-9d15-a5ccdff743fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "### Test\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_poicompute_OPTICSnts= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_OPTICS_clus, noise = compute_OPTICS(X_2, return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680f5ea-a2fd-4321-a773-5ac8395c3bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### compute_OPTICS\n",
       "\n",
       ">      compute_OPTICS (points2_clusters, **kwargs)\n",
       "\n",
       "*OPTICS wrapper.\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       ":returns:  list with numpy arrays for all the clusters obtained*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### compute_OPTICS\n",
       "\n",
       ">      compute_OPTICS (points2_clusters, **kwargs)\n",
       "\n",
       "*OPTICS wrapper.\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       ":returns:  list with numpy arrays for all the clusters obtained*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compute_OPTICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ebcbd-606e-4b8a-8597-7ffa3c34c53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb63891-9a59-4af6-b06a-366310d0053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def natural_cities_polygons(a_points, **kwargs ):\n",
    "    \"\"\" Take a array of points and returns the natural Cities polygons.\n",
    "        Parameters:\n",
    "        a_points (Array points): points to process into natural cities .\n",
    "        result_df (Polygon GeoDataframe): Single Geodataframe with the polygons that encloses the points\n",
    "    \"\"\"\n",
    "    tail_particion = kwargs.get('tail_particion', 0.55)\n",
    "    polygon= kwargs.get('polygon', None)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    edges = get_segments(a_points)\n",
    "    edges = {'geometry': edges}\n",
    "    edges_df = gpd.GeoDataFrame(edges)\n",
    "    edges_df['length'] = edges_df.geometry.length\n",
    "    mean_lenght=edges_df['length'].mean()\n",
    "    tail = edges_df[edges_df['length'] < mean_lenght]\n",
    "    ##check if is heavi tail\n",
    "    tail_percent = tail.shape[0]/edges_df.shape[0]\n",
    "    if tail_percent < tail_particion:\n",
    "        if debugg:\n",
    "            print('The percentage of edges that are in the tail are not meet, return empy dataframes ')\n",
    "        #### Returning empty dataframe\n",
    "        tail = gpd.GeoDataFrame()\n",
    "        result_df = gpd.GeoDataFrame()\n",
    "        return (tail, result_df)\n",
    "        \n",
    "    \n",
    "    if polygon is not None:\n",
    "        # use only lines within polygon\n",
    "        tail = gpd.sjoin(tail, polygon, how='inner', op='within')\n",
    "    linework = linemerge( tail.geometry.to_list() )\n",
    "    linework = unary_union(linework)\n",
    "    result, _, _, _ = polygonize_full(linework)\n",
    "    result = unary_union(result)\n",
    "    if type(result ) == shapely.Polygon:\n",
    "        gpd.GeoDataFrame(index=[0],geometry=[ result])\n",
    "    else:\n",
    "        result = {'geometry': result.geoms}\n",
    "    try:\n",
    "        result_df = gpd.GeoDataFrame(result)\n",
    "        \n",
    "    except:\n",
    "        print(result)\n",
    "        #print(len(result.geoms))\n",
    "        print('Unable to do the dataframe return empty dataframes ')\n",
    "        tail = gpd.GeoDataFrame()\n",
    "        result_df = gpd.GeoDataFrame()\n",
    "        return (tail, result_df)\n",
    "        \n",
    "        # result_df = gpd.GeoDataFrame({'geometry':[]})\n",
    "    return (tail, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38f745-62bd-4e8d-8b4e-8a9fdd1a4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def compute_Natural_cities(points2_clusters, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute Natural cities clustering\n",
    "    \n",
    "    :param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :returns: list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    ### The function is in acordance with the all the previus functions\n",
    "    scale_points= kwargs.get('scale_points',False)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    \n",
    "    \n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "\n",
    "    polygons_df = natural_cities_polygons(points_arr, **kwargs)\n",
    "    ####Handle when dataframes are empty, it could be for tail partition large enogh\n",
    "    ### empty  \n",
    "    if debugg:\n",
    "        print('Los poligonos de Natural cities')\n",
    "        print(polygons_df[1].head())\n",
    "    if polygons_df[0].empty and polygons_df[1].empty:\n",
    "        if debugg:\n",
    "            print('Not meeting the minimun tail size')\n",
    "        if ret_noise == True:\n",
    "            return [], points2_clusters\n",
    "        else:\n",
    "            return [] # return empty cluster\n",
    "\n",
    "    if debugg:\n",
    "        print('Size of the polygons dataframe: ', polygons_df[1].shape[0])\n",
    "    #### put the point in a data frame to do a spatial joint\n",
    "    all_points_Point= [shapely.geometry.Point(arre[0], arre[1]) for arre in  points2_clusters]\n",
    "\n",
    "    points_geom_df = gpd.GeoDataFrame({'geometry':all_points_Point })\n",
    "    result_joint  = gpd.sjoin(points_geom_df, polygons_df[1], how='left', predicate='within')\n",
    "    \n",
    "    result_joint= result_joint.fillna(-1)\n",
    "    result_joint['x']= result_joint['geometry'].x\n",
    "    result_joint['y']= result_joint['geometry'].y\n",
    "    \n",
    "    #### recover\n",
    "    if scale_points ==True:\n",
    "        array_trans = scaler.inverse_transform(result_joint[['x', 'y']].values)\n",
    "        result_joint['x_trans']=  array_trans[:,0]\n",
    "        result_joint['y_trans']=  array_trans[:,1]\n",
    "\n",
    "    else:\n",
    "        result_joint['x_trans']=  result_joint['x']\n",
    "        result_joint['y_trans']=  result_joint['y']\n",
    "    \n",
    "    if debugg:\n",
    "        print('The shape of the Dataframe of points: ', result_joint.shape)\n",
    "        \n",
    "    \n",
    "    if debugg:\n",
    "        print('Number of classes of  polygons: ',len(result_joint['index_right'].unique()))\n",
    "    \n",
    "    \n",
    "    clusters = []\n",
    "    for i in result_joint['index_right'].unique():\n",
    "        if i != -1:\n",
    "            clusters.append(result_joint[result_joint['index_right'] == i][['x_trans','y_trans']].values)\n",
    "        else:\n",
    "            noise_ = result_joint[result_joint['index_right'] == i][['x_trans','y_trans']].values\n",
    "    \n",
    "    if ret_noise == True:\n",
    "        return clusters, noise_\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7b0c4-19c3-4873-8c50-7d686224a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "### Test\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_, noise = compute_Natural_cities(X_2, return_noise = True, debugg= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7693290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27846843, 0.80914781],\n",
       "       [0.52391084, 0.59325509],\n",
       "       [0.65483759, 0.24310224],\n",
       "       [0.50474789, 0.82015235],\n",
       "       [0.53004998, 0.33698684],\n",
       "       [0.2495555 , 0.29149051],\n",
       "       [0.09089007, 0.81312556],\n",
       "       [0.18152801, 0.77747124],\n",
       "       [0.56555912, 0.77291516],\n",
       "       [0.22225166, 0.27143518],\n",
       "       [0.28203165, 0.51794488],\n",
       "       [0.44481324, 0.37593993],\n",
       "       [0.79067309, 0.55425223],\n",
       "       [0.87343423, 0.39056198],\n",
       "       [0.22977206, 0.67220829],\n",
       "       [0.26317959, 0.42042386],\n",
       "       [0.5114688 , 0.46629523],\n",
       "       [0.41841972, 0.79401155],\n",
       "       [0.83385101, 0.6633833 ],\n",
       "       [0.68686791, 0.5503606 ],\n",
       "       [0.46587041, 0.67448433],\n",
       "       [0.37225114, 0.78426381],\n",
       "       [0.41587898, 0.61443469],\n",
       "       [0.86905387, 0.35086021],\n",
       "       [0.72912573, 0.3152534 ],\n",
       "       [0.76107694, 0.2536277 ],\n",
       "       [0.36773862, 0.73177468],\n",
       "       [0.30432014, 0.54789816],\n",
       "       [0.5294655 , 0.37948529],\n",
       "       [0.18057029, 0.70411488],\n",
       "       [0.27378111, 0.4977626 ],\n",
       "       [0.64845132, 0.26339484],\n",
       "       [0.67419108, 0.47210101],\n",
       "       [0.62007521, 0.68014506],\n",
       "       [0.08286733, 0.84531576],\n",
       "       [0.20195261, 0.68103464],\n",
       "       [0.46052824, 0.72851995],\n",
       "       [0.17143993, 0.38170532],\n",
       "       [0.55773173, 0.77326758],\n",
       "       [0.5257344 , 0.57170635],\n",
       "       [0.5983236 , 0.73777284],\n",
       "       [0.63409442, 0.75937461],\n",
       "       [0.61259247, 0.75488265],\n",
       "       [0.61655395, 0.75196764],\n",
       "       [0.59472456, 0.7330525 ],\n",
       "       [0.61182595, 0.75961749],\n",
       "       [0.61847573, 0.73127937],\n",
       "       [0.6052646 , 0.72869992],\n",
       "       [0.59404029, 0.74570391],\n",
       "       [0.61048834, 0.73064882],\n",
       "       [0.61267372, 0.73226498],\n",
       "       [0.63342424, 0.76080096],\n",
       "       [0.61635471, 0.74591278],\n",
       "       [0.63341994, 0.73952073],\n",
       "       [0.63184632, 0.75728377],\n",
       "       [0.62055819, 0.73030384],\n",
       "       [0.61674319, 0.75393018],\n",
       "       [0.62766014, 0.73318275],\n",
       "       [0.61977615, 0.75009389],\n",
       "       [0.63106601, 0.74713385],\n",
       "       [0.62759864, 0.73585642],\n",
       "       [0.6131302 , 0.72867864],\n",
       "       [0.62952259, 0.75759276],\n",
       "       [0.60132348, 0.7273452 ],\n",
       "       [0.61970524, 0.74058452],\n",
       "       [0.61919991, 0.74236322],\n",
       "       [0.59277908, 0.72857878],\n",
       "       [0.60830068, 0.72201519],\n",
       "       [0.61945698, 0.73540784],\n",
       "       [0.90002111, 0.44482563],\n",
       "       [0.89049038, 0.45183174],\n",
       "       [0.89229064, 0.45448481],\n",
       "       [0.89956935, 0.4427603 ],\n",
       "       [0.89556471, 0.45460977],\n",
       "       [0.89728441, 0.44141429],\n",
       "       [0.89601617, 0.44243409],\n",
       "       [0.8982268 , 0.44910872],\n",
       "       [0.89831643, 0.44176192],\n",
       "       [0.89808865, 0.45428368],\n",
       "       [0.90021141, 0.45349059],\n",
       "       [0.89351018, 0.44769604],\n",
       "       [0.88887632, 0.4462981 ],\n",
       "       [0.63386872, 0.73358659],\n",
       "       [0.6333008 , 0.73344867],\n",
       "       [0.63387994, 0.73426457],\n",
       "       [0.63322182, 0.73438282],\n",
       "       [0.633453  , 0.73344174],\n",
       "       [0.63358015, 0.73346319],\n",
       "       [0.62210592, 0.72970642],\n",
       "       [0.62278295, 0.72954001],\n",
       "       [0.62270369, 0.73075907],\n",
       "       [0.60710179, 0.73020663],\n",
       "       [0.60678074, 0.72690157],\n",
       "       [0.60678998, 0.73003409],\n",
       "       [0.60996749, 0.7447214 ],\n",
       "       [0.61352645, 0.744276  ],\n",
       "       [0.60666835, 0.7418374 ],\n",
       "       [0.60743264, 0.74508127],\n",
       "       [0.60862519, 0.73807517],\n",
       "       [0.60628171, 0.73896912],\n",
       "       [0.60850343, 0.74533274],\n",
       "       [0.60691611, 0.74396202],\n",
       "       [0.60980668, 0.72737477],\n",
       "       [0.61047429, 0.7378374 ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c071ebd-4dba-414d-bf72-c1dd4f84566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### compute_Natural_cities\n",
       "\n",
       ">      compute_Natural_cities (points2_clusters, **kwargs)\n",
       "\n",
       "*Compute Natural cities clustering\n",
       "\n",
       ":param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns: list with numpy arrays for all the clusters obtained*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### compute_Natural_cities\n",
       "\n",
       ">      compute_Natural_cities (points2_clusters, **kwargs)\n",
       "\n",
       "*Compute Natural cities clustering\n",
       "\n",
       ":param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns: list with numpy arrays for all the clusters obtained*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compute_Natural_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43396daa-55e9-4819-929c-b8af3954f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def compute_AMOEBA(points_array, **kwargs):\n",
    "    \"\"\"The function obtains the AMOEBA algorithm on level basis\n",
    "    \n",
    "    :param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :returns: list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    \n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    min_leng_clus_AMOEBA= kwargs.get('min_lenght_cluster_AMOEBA', 3)\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points_array)\n",
    "    else:\n",
    "        points_arr = points_array\n",
    "    ########\n",
    "    if len(points_arr) < min_leng_clus_AMOEBA:\n",
    "        clusters=[]\n",
    "        noise_level= np.empty((0,2))\n",
    "        if ret_noise == True:\n",
    "            return clusters, noise_level\n",
    "        else:\n",
    "            return clusters\n",
    "    \n",
    "    \n",
    "    gr, pos_d =triangulation(points_arr, \"delaunay\")\n",
    "    dis_d = gr.new_edge_property(\"double\")\n",
    "    for e in gr.edges():\n",
    "        dis_d[e] =  norm(pos_d[e.target()].a - pos_d[e.source()].a)\n",
    "    gr.edge_properties[\"dis\"] = dis_d\n",
    "    gr.vertex_properties[\"pos\"] = pos_d\n",
    "    global_edge_mean= np.nan_to_num(gr.edge_properties['dis'].get_array().mean())\n",
    "    global_edge_std = np.nan_to_num(gr.edge_properties['dis'].get_array().std() )\n",
    "    \n",
    "    all_remove_level =[]\n",
    "    all_keep_level = []\n",
    "    for vert in gr.vertices():\n",
    "        local_mean= np.mean([gr.edge_properties['dis'][vo_edge]  for vo_edge in vert.out_edges()])\n",
    "        tolerance = global_edge_std * (global_edge_mean/local_mean)\n",
    "        rem_edg_loc = []\n",
    "        keep_edg_loc = []\n",
    "        for ed in vert.all_edges():\n",
    "            if gr.edge_properties['dis'][ed] > tolerance + global_edge_mean:\n",
    "                rem_edg_loc.append(ed)\n",
    "            else:\n",
    "                keep_edg_loc.append(ed)\n",
    "        all_keep_level.append(keep_edg_loc)\n",
    "        all_remove_level.append(rem_edg_loc)\n",
    "    \n",
    "    all_remove_level_flat= []\n",
    "    for _list in all_remove_level:\n",
    "        all_remove_level_flat += _list\n",
    "    all_keep_level_flat= []\n",
    "    for _list in all_keep_level:\n",
    "        all_keep_level_flat += _list\n",
    "    level_n = gr.new_edge_property(\"bool\", True)\n",
    "    gr.edge_properties[\"level_n_tolerance\"] = level_n\n",
    "    \n",
    "    #### Probably not needed or can be reduce\n",
    "    #### The edge tolerance\n",
    "    for i in all_remove_level_flat:\n",
    "        gr.edge_properties['level_n_tolerance'][i]= False\n",
    "    for i in all_keep_level_flat:\n",
    "        gr.edge_properties['level_n_tolerance'][i]= True\n",
    "        \n",
    "    gr.set_edge_filter(prop =  gr.edge_properties['level_n_tolerance'])\n",
    "    \n",
    "    \n",
    "    ##### If the vertex should be keep\n",
    "    gr.vertex_properties[\"level_n_r\"] = gr.new_vertex_property(\"bool\", False)\n",
    "    for vert in gr.vertices():\n",
    "        if vert.in_degree() + vert.out_degree()> 0:\n",
    "            gr.vertex_properties['level_n_r'][vert]= True\n",
    "        else: \n",
    "            gr.vertex_properties['level_n_r'][vert]= False\n",
    "    ##  to not consider the noise points\n",
    "    gr.set_vertex_filter(prop =  gr.vertex_properties['level_n_r'])\n",
    "    \n",
    "    ## Get the connected components\n",
    "    level_n_components_arr, comp_n_hist = label_components(gr)\n",
    "    gr.set_vertex_filter(None)\n",
    "    \n",
    "    gr.vertex_properties[\"compo_level_n\"] = gr.new_vertex_property(\"int\", -1)\n",
    "    \n",
    "    gr.set_vertex_filter(prop =  gr.vertex_properties['level_n_r'])\n",
    "    \n",
    "    \n",
    "    compo_level_res_n = gr.new_vertex_property(\"int\", -1)\n",
    "    compo_level_res_n.a = level_n_components_arr.a\n",
    "    gr.vertex_properties[\"compo_level_res_n\"] = compo_level_res_n\n",
    "    \n",
    "    for vert in gr.vertices():\n",
    "        # print(num)\n",
    "        if vert.in_degree() + vert.out_degree()> 0:\n",
    "            gr.vertex_properties['compo_level_n'][vert]= gr.vertex_properties[\"compo_level_res_n\"][vert] \n",
    "        else: \n",
    "            # print('No edge')\n",
    "            gr.vertex_properties['compo_level_n'][vert]= -1\n",
    "    \n",
    "    \n",
    "    ####### get the points for each cluster \n",
    "    clusters_result_n= np.nan_to_num(np.unique(gr.vertex_properties['compo_level_n'].a))\n",
    "    clusters=[]\n",
    "    noise_level= np.empty((0,2))\n",
    "    #######check that not returning the same cluster \n",
    "    if len(clusters_result_n) == 1 and len(points_array) == sum(gr.vertex_properties['compo_level_n'].a  == 0):\n",
    "        if debugg:\n",
    "            print('Its the same set of points after clustering')\n",
    "            print('Only one cluster with the same number of points \\n')\n",
    "            print('Returns the points as noise')\n",
    "        \n",
    "        if ret_noise == True:\n",
    "            class_mask = (gr.vertex_properties['compo_level_n'].a  == 0)\n",
    "            return clusters, points_array[class_mask] #Empty cluster list an all the points as noise\n",
    "        else:\n",
    "            return clusters # return empty cluster\n",
    "    \n",
    "    ########\n",
    "\n",
    "    \n",
    "    \n",
    "    for clas in clusters_result_n :\n",
    "        if clas != -1:\n",
    "            clas_mask = ( gr.vertex_properties['compo_level_n'].a == clas)\n",
    "            clusters.append(points_array[clas_mask])\n",
    "        else:\n",
    "            clas_mask = ( gr.vertex_properties['compo_level_n'].a == clas)\n",
    "            noise_level= points_array[clas_mask]\n",
    "    if ret_noise == True:\n",
    "        return clusters, noise_level\n",
    "    return clusters\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f50da1-2aef-40ba-bc3f-7aaf11cbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "### Test\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "res_AMOEBA_clus, noise = compute_AMOEBA(X_2, return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257f982-325f-4a8c-a661-ae988ba50041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### compute_AMOEBA\n",
       "\n",
       ">      compute_AMOEBA (points_array, **kwargs)\n",
       "\n",
       "*The function obtains the AMOEBA algorithm on level basis\n",
       "\n",
       ":param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns: list with numpy arrays for all the clusters obtained*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### compute_AMOEBA\n",
       "\n",
       ">      compute_AMOEBA (points_array, **kwargs)\n",
       "\n",
       "*The function obtains the AMOEBA algorithm on level basis\n",
       "\n",
       ":param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns: list with numpy arrays for all the clusters obtained*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compute_AMOEBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf53fec-f48b-46c9-847b-afbfc814ed55",
   "metadata": {},
   "source": [
    "## Tree and clustering \n",
    "The following functions use the clustering functions define abobe to clasify and obtain a cluster tree structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadca08-8cee-4a27-9be1-ae519358afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clustering(\n",
    "            t_next_level_2,\n",
    "            level=None,\n",
    "            algorithm='dbscan',\n",
    "            **kwargs\n",
    "    ):\n",
    "    \"\"\"Function to get the clusters for single group by\n",
    "    \n",
    "    :param t_next_level_2: Dictionary with the points to compute the\n",
    "            cluster\n",
    "    :param level:  None Level to compute (Default None)\n",
    "    \n",
    "    :param str algorithm : Algorithm type is supported (Default= 'dbscan')\n",
    "    \n",
    "    :param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)\n",
    "    \n",
    "    :param double eps: Epsilon parameter in the case that is needed (DBSCAN).\n",
    "    \n",
    "    :param bool return_noise: To return the noise (Default= True)\n",
    "    \n",
    "    :param bool verbose: Printing (Default= False)  \n",
    "    \n",
    "    :param algorithm_pass function: the algorithm to use if algorith = 'other',\n",
    "            the algorithm has to be in acordance to return the same as \n",
    "            all other algorithms implemented. \n",
    "    \n",
    "    \n",
    "    :returns list t_next_level_n: A list with dictionaries with the points, \n",
    "                        the parent, and noise\n",
    "    \"\"\"\n",
    "    verbose= kwargs.get('verbose',False)\n",
    "    min_points = kwargs.get( 'min_points_cluster', 50) #### creo que se deberia quitar o poner bien\n",
    "    ret_noise= kwargs.get('return_noise', True)\n",
    "    eps = kwargs.get('eps',0.8)  # Epsilon value to dbscan\n",
    "    algorithm_pass= kwargs.get('algorithm_pass', None)\n",
    "\n",
    "    min_leng_clus= kwargs.get('min_lenght_cluster', 5)\n",
    "    t_next_level_n = []\n",
    "    if level == None:\n",
    "        level = 0\n",
    "\n",
    "    for li_num, cluster_list_D in enumerate(t_next_level_2):\n",
    "        cluster_list = cluster_list_D['points']\n",
    "        cluster_list_pa = cluster_list_D['parent']\n",
    "        if verbose:\n",
    "            print(\"Size cluster list: \", len(cluster_list))\n",
    "            \n",
    "        for c_num, cluster in enumerate(cluster_list):\n",
    "            if verbose:\n",
    "                print(\"Size cluster: \", len(cluster))\n",
    "                print('Algorithm: ', algorithm)\n",
    "\n",
    "            if len(cluster) > min_leng_clus:\n",
    "                if algorithm == 'dbscan':\n",
    "                    if verbose:\n",
    "                        print(\"Epsilon Value: \", eps)\n",
    "                    tmp = compute_dbscan(cluster,\n",
    "                                 eps_DBSCAN = eps,\n",
    "                                 debugg=verbose,\n",
    "                                  **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                \n",
    "               \n",
    "                elif algorithm == 'hdbscan':\n",
    "                    tmp = compute_hdbscan(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                elif algorithm == 'adaptative_DBSCAN':\n",
    "                    #### If the number of cluster is too small \n",
    "                    \n",
    "                    tmp = adaptative_DBSCAN(cluster, **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "\n",
    "                elif algorithm == 'optics':\n",
    "                    tmp = compute_OPTICS(cluster,\n",
    "                                eps_OPTICS = eps,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                elif algorithm == 'natural_cities':\n",
    "                    tmp = compute_Natural_cities(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                elif algorithm == 'amoeba':\n",
    "                    tmp = compute_AMOEBA(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                #########\n",
    "                \n",
    "                elif algorithm == 'other' and algorithm_pass != None:\n",
    "                    tmp = algorithm_pass(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                #########        \n",
    "                else:\n",
    "                    raise ValueError('Algorithm must be: \\n', \n",
    "                                     'dbscan, hdbscan, adaptative_DBSCAN, optics, natural_cities or amoeba')\n",
    "                    # sys.exit(\"1\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"The number of resulting clusters is : \", len(tmp))\n",
    "                if ret_noise:\n",
    "                    dic_clos = {'points': tmp,\n",
    "                           'parent': cluster_list_pa + '_L_'+str(level) +\n",
    "                            '_l_' + str(li_num) + '_c_'+str(c_num), \n",
    "                            'noise_points':noise_points\n",
    "                    }\n",
    "                else:\n",
    "                    dic_clos = {'points': tmp, 'parent': cluster_list_pa +\n",
    "                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}\n",
    "                \n",
    "                t_next_level_n.append(dic_clos)\n",
    "            else:\n",
    "                if ret_noise:\n",
    "                    dic_clos = {'points': [],\n",
    "                           'parent': cluster_list_pa + '_L_'+str(level) +\n",
    "                            '_l_' + str(li_num) + '_c_'+str(c_num), \n",
    "                            'noise_points':cluster\n",
    "                    }\n",
    "                else:\n",
    "                    dic_clos = {'points': [], 'parent': cluster_list_pa +\n",
    "                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}\n",
    "                t_next_level_n.append(dic_clos)\n",
    "    \n",
    "    return t_next_level_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7a559-9d82-4dcc-bd65-0719025bbaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'points': [array([[0.4971626 , 0.14070677],\n",
       "          [0.46563505, 0.18158396],\n",
       "          [0.48771552, 0.10108305],\n",
       "          ...,\n",
       "          [0.52775176, 0.0803683 ],\n",
       "          [0.52775027, 0.08036795],\n",
       "          [0.52775153, 0.08036426]])],\n",
       "  'parent': '_L_0_l_0_c_0',\n",
       "  'noise_points': array([[0.73828447, 0.21114412],\n",
       "         [0.59859979, 0.25559214],\n",
       "         [0.25031195, 0.73863799],\n",
       "         [0.28174407, 0.87012264],\n",
       "         [0.15705896, 0.69769038],\n",
       "         [0.65851959, 0.17443232],\n",
       "         [0.27425068, 0.46154958],\n",
       "         [0.54903556, 0.28846742],\n",
       "         [0.19765106, 0.67086148],\n",
       "         [0.30829488, 0.80201824],\n",
       "         [0.65327092, 0.52144383],\n",
       "         [0.40289586, 0.26939394],\n",
       "         [0.32720683, 0.6914416 ],\n",
       "         [0.31670266, 0.51646512],\n",
       "         [0.26400494, 0.48292039],\n",
       "         [0.7249336 , 0.67390583],\n",
       "         [0.34082068, 0.65997417],\n",
       "         [0.72701393, 0.13074379],\n",
       "         [0.25103418, 0.52698407],\n",
       "         [0.31385628, 0.6995318 ],\n",
       "         [0.64680457, 0.68700507],\n",
       "         [0.74260337, 0.20185671],\n",
       "         [0.22620297, 0.35087134],\n",
       "         [0.05537205, 0.61576511],\n",
       "         [0.43021483, 0.12000754],\n",
       "         [0.69040267, 0.07279018],\n",
       "         [0.50286138, 0.29846275],\n",
       "         [0.27352662, 0.12234843],\n",
       "         [0.50645386, 0.41611709],\n",
       "         [0.23743977, 0.83496438],\n",
       "         [0.37369825, 0.8055583 ],\n",
       "         [0.76188466, 0.44777469],\n",
       "         [0.83478445, 0.06276282],\n",
       "         [0.66545955, 0.35161865],\n",
       "         [0.40163544, 0.6573924 ],\n",
       "         [0.57926798, 0.59561176],\n",
       "         [0.52963793, 0.54039446],\n",
       "         [0.53617135, 0.50155447],\n",
       "         [0.58092399, 0.50526807],\n",
       "         [0.53827337, 0.56460492],\n",
       "         [0.52120058, 0.57062018],\n",
       "         [0.57746874, 0.52635171],\n",
       "         [0.61536888, 0.58791677],\n",
       "         [0.59219153, 0.56132153],\n",
       "         [0.58924134, 0.51411272],\n",
       "         [0.55595356, 0.49863533],\n",
       "         [0.55052143, 0.49875418],\n",
       "         [0.56280218, 0.52031405],\n",
       "         [0.5470549 , 0.49280434],\n",
       "         [0.60577074, 0.52311881],\n",
       "         [0.59066066, 0.57351372],\n",
       "         [0.52487669, 0.53429428],\n",
       "         [0.53029126, 0.50628024],\n",
       "         [0.59631406, 0.56982562],\n",
       "         [0.55405322, 0.61924904],\n",
       "         [0.55334261, 0.54805815],\n",
       "         [0.59487689, 0.5525348 ],\n",
       "         [0.58819014, 0.53734926],\n",
       "         [0.61674454, 0.58467281],\n",
       "         [0.51025546, 0.55855607],\n",
       "         [0.51242478, 0.50187018],\n",
       "         [0.60003957, 0.50424161],\n",
       "         [0.5498867 , 0.59503055],\n",
       "         [0.56797455, 0.6271199 ],\n",
       "         [0.53481246, 0.51227255],\n",
       "         [0.60114474, 0.52052623],\n",
       "         [0.60321269, 0.51190094],\n",
       "         [0.56092093, 0.50333676],\n",
       "         [0.61509982, 0.56384654],\n",
       "         [0.57057371, 0.60533495],\n",
       "         [0.54017907, 0.5958737 ],\n",
       "         [0.52703613, 0.5691008 ],\n",
       "         [0.52186418, 0.56482423],\n",
       "         [0.52990251, 0.5164115 ],\n",
       "         [0.60227078, 0.57778653]])}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "#|echo: False\n",
    "### Pruebas \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "clustering([dic_points], return_noise = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb807e-6bd7-47f7-904d-ba6448872f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### clustering\n",
       "\n",
       ">      clustering (t_next_level_2, level=None, algorithm='dbscan', **kwargs)\n",
       "\n",
       "*Function to get the clusters for single group by\n",
       "\n",
       ":param t_next_level_2: Dictionary with the points to compute the\n",
       "        cluster\n",
       ":param level:  None Level to compute (Default None)\n",
       "\n",
       ":param str algorithm : Algorithm type is supported (Default= 'dbscan')\n",
       "\n",
       ":param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)\n",
       "\n",
       ":param double eps: Epsilon parameter in the case that is needed (DBSCAN).\n",
       "\n",
       ":param bool return_noise: To return the noise (Default= True)\n",
       "\n",
       ":param bool verbose: Printing (Default= False)  \n",
       "\n",
       ":param algorithm_pass function: the algorithm to use if algorith = 'other',\n",
       "        the algorithm has to be in acordance to return the same as \n",
       "        all other algorithms implemented. \n",
       "\n",
       ":returns list t_next_level_n: A list with dictionaries with the points, \n",
       "                    the parent, and noise*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### clustering\n",
       "\n",
       ">      clustering (t_next_level_2, level=None, algorithm='dbscan', **kwargs)\n",
       "\n",
       "*Function to get the clusters for single group by\n",
       "\n",
       ":param t_next_level_2: Dictionary with the points to compute the\n",
       "        cluster\n",
       ":param level:  None Level to compute (Default None)\n",
       "\n",
       ":param str algorithm : Algorithm type is supported (Default= 'dbscan')\n",
       "\n",
       ":param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)\n",
       "\n",
       ":param double eps: Epsilon parameter in the case that is needed (DBSCAN).\n",
       "\n",
       ":param bool return_noise: To return the noise (Default= True)\n",
       "\n",
       ":param bool verbose: Printing (Default= False)  \n",
       "\n",
       ":param algorithm_pass function: the algorithm to use if algorith = 'other',\n",
       "        the algorithm has to be in acordance to return the same as \n",
       "        all other algorithms implemented. \n",
       "\n",
       ":returns list t_next_level_n: A list with dictionaries with the points, \n",
       "                    the parent, and noise*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadc0e6-da7b-4d25-9099-d45d05659d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recursive_clustering(\n",
    "                this_level,  # Dictionary with Points\n",
    "                to_process,  # levels to process\n",
    "                cluster_tree,  # to store the clusters\n",
    "                level = 0,  # current level\n",
    "                **kwargs\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Performs the recursive clustering.\n",
    "    Calls compute_dbscan for each\n",
    "    list of clusters keepen the structure and then calls itself\n",
    "    until no more clusters satisfy the condition\n",
    "        \n",
    "    :param dict this_level: level is the current level \n",
    "    \n",
    "    :param int to_process: the max level to process\n",
    "    \n",
    "    :param double eps: The epsilon parameter distance to pass to the needed algorithm \n",
    "    \n",
    "    :param list cluster_tree : list of list to insert the levels \n",
    "    \n",
    "    :param bool verbose : To print \n",
    "    \n",
    "    :param double decay: In the use of dbscan the deacy parameter to reduce eps\n",
    "    \n",
    "    :param int min_points_cluster: The min point for each cluster to pass to algorithm\n",
    "    \n",
    "    :param str algorithm:  The string of the algorithm name to use\n",
    "    \"\"\"\n",
    "    algorithm= kwargs.get('algorithm' ,'dbscan')    # Algorithm to use\n",
    "    verbose= kwargs.get('verbose',False)\n",
    "    min_points = kwargs.get( 'min_points_cluster', 50)\n",
    "    decay = kwargs.get('decay', 0.7)\n",
    "    eps = kwargs.get('eps' ,0.8)  # Epsilon distance to DBSCAN parameter\n",
    "    max_k_increase = kwargs.get('max_k_increase', None)\n",
    "    tmp = None\n",
    "\n",
    "    if level == 0:\n",
    "        kwargs['eps'] = eps\n",
    "    else:\n",
    "        kwargs['eps'] = eps  * decay\n",
    "\n",
    "    if max_k_increase != None:\n",
    "        if level == 0:\n",
    "            kwargs['max_k_percent'] = 0.1\n",
    "        else:\n",
    "            kwargs['max_k_percent'] = kwargs['max_k_percent'] * max_k_increase\n",
    "    \n",
    "    cluster_result_polygons = []\n",
    "    if level > to_process:\n",
    "        if verbose:\n",
    "            print('Done clustering')\n",
    "        return\n",
    "    ######## Get the clusters for the current list of points \n",
    "    all_l = clustering(\n",
    "                    this_level,\n",
    "                    level=level,                    \n",
    "                    **kwargs\n",
    "                    )\n",
    "    ##########\n",
    "\n",
    "    cluster_tree.append(all_l)\n",
    "    cluster_n = 0\n",
    "    for i in all_l:\n",
    "        cluster_n += len(i['points'])\n",
    "    if verbose:\n",
    "        print('At level ', level, ' the number of lists are ',\n",
    "              len(all_l), ' with ', cluster_n, 'clusters')\n",
    "    level += 1\n",
    "    if len(all_l) > 0:\n",
    "        return recursive_clustering(all_l, \n",
    "                               to_process=to_process,\n",
    "                               cluster_tree=cluster_tree,\n",
    "                               level= level,\n",
    "                               **kwargs\n",
    "                               )\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('done clustering')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372eacfa-9a9f-4485-bcfb-95710f57cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "#|echo: False\n",
    "### Pruebas \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "levels_clustering_t= 4\n",
    "cluster_tree_t = []\n",
    "recursive_clustering([dic_points],  # Dictionary with Points\n",
    "                levels_clustering_t,  # levels to process\n",
    "                cluster_tree_t,  # to store the clusters\n",
    "                level=0,  # current level\n",
    "                     \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e05b61-f672-4d5f-b085-439d0694490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### recursive_clustering\n",
       "\n",
       ">      recursive_clustering (this_level, to_process, cluster_tree, level=0,\n",
       ">                            **kwargs)\n",
       "\n",
       "*Performs the recursive clustering.\n",
       "Calls compute_dbscan for each\n",
       "list of clusters keepen the structure and then calls itself\n",
       "until no more clusters satisfy the condition\n",
       "\n",
       ":param dict this_level: level is the current level \n",
       "\n",
       ":param int to_process: the max level to process\n",
       "\n",
       ":param double eps: The epsilon parameter distance to pass to the needed algorithm \n",
       "\n",
       ":param list cluster_tree : list of list to insert the levels \n",
       "\n",
       ":param bool verbose : To print \n",
       "\n",
       ":param double decay: In the use of dbscan the deacy parameter to reduce eps\n",
       "\n",
       ":param int min_points_cluster: The min point for each cluster to pass to algorithm\n",
       "\n",
       ":param str algorithm:  The string of the algorithm name to use*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| this_level |  |  | Dictionary with Points |\n",
       "| to_process |  |  | levels to process |\n",
       "| cluster_tree |  |  | to store the clusters |\n",
       "| level | int | 0 | current level |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### recursive_clustering\n",
       "\n",
       ">      recursive_clustering (this_level, to_process, cluster_tree, level=0,\n",
       ">                            **kwargs)\n",
       "\n",
       "*Performs the recursive clustering.\n",
       "Calls compute_dbscan for each\n",
       "list of clusters keepen the structure and then calls itself\n",
       "until no more clusters satisfy the condition\n",
       "\n",
       ":param dict this_level: level is the current level \n",
       "\n",
       ":param int to_process: the max level to process\n",
       "\n",
       ":param double eps: The epsilon parameter distance to pass to the needed algorithm \n",
       "\n",
       ":param list cluster_tree : list of list to insert the levels \n",
       "\n",
       ":param bool verbose : To print \n",
       "\n",
       ":param double decay: In the use of dbscan the deacy parameter to reduce eps\n",
       "\n",
       ":param int min_points_cluster: The min point for each cluster to pass to algorithm\n",
       "\n",
       ":param str algorithm:  The string of the algorithm name to use*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| this_level |  |  | Dictionary with Points |\n",
       "| to_process |  |  | levels to process |\n",
       "| cluster_tree |  |  | to store the clusters |\n",
       "| level | int | 0 | current level |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(recursive_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a050fa-31b3-4d9f-bf4e-bde5a859353e",
   "metadata": {},
   "source": [
    "## Clustering and hieralchical structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641868a1-0290-473f-ad20-688ab8635ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tree_from_clustering(cluster_tree_clusters):\n",
    "    \"\"\" Returns the tree from the iterative clustering, the cluster_tree_cluster\n",
    "     \n",
    "     :param cluster_tree_clusters is a list of list with a dictionary that \n",
    "            should contain the points of the next level clusters, the name of the parent\n",
    "            cluster of such clusters (name of the current node), and the point that \n",
    "            are consider noise. \n",
    "    \n",
    "     :return A list of list that contains the nodes for each level of the tree. \n",
    "    \"\"\" \n",
    "     ##### La estructura de arbol \n",
    "    all_level_clusters =[]\n",
    "    previus_level=[]\n",
    "    list_len = len(cluster_tree_clusters)-1\n",
    "    for level_num_clus, level_cluster in enumerate(cluster_tree_clusters):\n",
    "        level_nodes=[]\n",
    "        for cluster_te in level_cluster:\n",
    "            node_l = NodeCluster(name= cluster_te['parent'])\n",
    "            to_concat= [point_arr for point_arr in cluster_te['points']]\n",
    "            if len(to_concat)>0:\n",
    "                points_poly = np.concatenate(to_concat)\n",
    "            else:\n",
    "                points_poly = np.array([], dtype= np.float64).reshape(0,2)\n",
    "               \n",
    "            points_poly = np.concatenate(\n",
    "                                   (points_poly,cluster_te['noise_points']),\n",
    "                                   axis=0\n",
    "                              )\n",
    "            \n",
    "            \n",
    "            ####### NOt a posible cluster \n",
    "            if len(points_poly) <3:\n",
    "                node_l.polygon_cluster = None\n",
    "            else:\n",
    "                try:\n",
    "                    node_l.polygon_cluster = get_alpha_shape(points_poly)\n",
    "                except:\n",
    "                    print('Unable to create the polygon returning None')\n",
    "                    node_l.polygon_cluster = None\n",
    "                \n",
    "            \n",
    "            ##### Es necesario que si es el ltimo nivel todos los puntos sean\n",
    "            ## considerados como ruido pues aunque se haya hecho la clusterizacion\n",
    "            #  ya no se bajo al siguiente nivel\n",
    "            ## \n",
    "            if level_num_clus==list_len:\n",
    "                node_l.point_cluster_noise = shapely.geometry.MultiPoint(points_poly) \n",
    "            else:\n",
    "                node_l.point_cluster_noise = shapely.geometry.MultiPoint(cluster_te['noise_points']) \n",
    "            \n",
    "            pos=node_l.name.rfind('_L')\n",
    "            if node_l.name[:pos]=='':\n",
    "                node_l.parent = None\n",
    "            else:\n",
    "                lis_pa=[item  for item in previus_level if item.name == node_l.name[:pos]]\n",
    "                node_l.parent= lis_pa[0]\n",
    "            level_nodes.append(node_l)\n",
    "        all_level_clusters.append(level_nodes)\n",
    "        previus_level = level_nodes\n",
    "    \n",
    "    return  all_level_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ee937-241d-47c5-8144-444cd41286f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<HierarchicalGeoClustering.TreeClusters.NodeCluster>],\n",
       " [<HierarchicalGeoClustering.TreeClusters.NodeCluster>],\n",
       " [<HierarchicalGeoClustering.TreeClusters.NodeCluster>,\n",
       "  <HierarchicalGeoClustering.TreeClusters.NodeCluster>],\n",
       " [<HierarchicalGeoClustering.TreeClusters.NodeCluster>,\n",
       "  <HierarchicalGeoClustering.TreeClusters.NodeCluster>,\n",
       "  <HierarchicalGeoClustering.TreeClusters.NodeCluster>,\n",
       "  <HierarchicalGeoClustering.TreeClusters.NodeCluster>],\n",
       " [<HierarchicalGeoClustering.TreeClusters.NodeCluster>]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "#|echo: False\n",
    "### Pruebas \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "levels_clustering_t= 4\n",
    "cluster_tree_t = []\n",
    "recursive_clustering([dic_points],  # Dictionary with Points\n",
    "                levels_clustering_t,  # levels to process\n",
    "                cluster_tree_t,  # to store the clusters\n",
    "                level=0,  # current level\n",
    "                     \n",
    "                )\n",
    "tree_clus= get_tree_from_clustering(cluster_tree_t)\n",
    "tree_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d1c1f-2de6-47c3-8d65-28d38ae9c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_tree_from_clustering\n",
       "\n",
       ">      get_tree_from_clustering (cluster_tree_clusters)\n",
       "\n",
       "*Returns the tree from the iterative clustering, the cluster_tree_cluster\n",
       "\n",
       ":param cluster_tree_clusters is a list of list with a dictionary that \n",
       "       should contain the points of the next level clusters, the name of the parent\n",
       "       cluster of such clusters (name of the current node), and the point that \n",
       "       are consider noise. \n",
       "\n",
       ":return A list of list that contains the nodes for each level of the tree.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_tree_from_clustering\n",
       "\n",
       ">      get_tree_from_clustering (cluster_tree_clusters)\n",
       "\n",
       "*Returns the tree from the iterative clustering, the cluster_tree_cluster\n",
       "\n",
       ":param cluster_tree_clusters is a list of list with a dictionary that \n",
       "       should contain the points of the next level clusters, the name of the parent\n",
       "       cluster of such clusters (name of the current node), and the point that \n",
       "       are consider noise. \n",
       "\n",
       ":return A list of list that contains the nodes for each level of the tree.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_tree_from_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192566b8-b38c-4fd2-b133-3cee470cf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def recursive_clustering_tree(dic_points_ori, **kwargs):\n",
    "    \"\"\"\n",
    "    Obtaing the recursive tree using a specific algorithm\n",
    "    \n",
    "    \n",
    "    :param dict dic_points_ori: A dictionary with two keys 'points':['Array points'],\n",
    "                                'parent':'name_parent' \n",
    "    :param int levels_clustering: levels to cluster  \n",
    "    :returns TreeClusters: wih the clusters as nodes\n",
    "    \"\"\"\n",
    "    levels_clustering= kwargs.get('levels_clustering',4)\n",
    "    cluster_tree = []\n",
    "    recursive_clustering([dic_points_ori],  # Dictionary with Points\n",
    "                levels_clustering,  # levels to process\n",
    "                cluster_tree,  # to store the clusters\n",
    "                level=0,  # current level\n",
    "                **kwargs\n",
    "                )\n",
    "    tree_clus= get_tree_from_clustering(cluster_tree)\n",
    "    tree_from_clus= TreeClusters()\n",
    "    tree_from_clus.levels_nodes = tree_clus\n",
    "    tree_from_clus.root= tree_from_clus.levels_nodes[0][0]   \n",
    "    return tree_from_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dba49-a20f-42c2-b82d-6264d86fdcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HierarchicalGeoClustering.TreeClusters.TreeClusters>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "### Pruebas \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "\n",
    "########## \n",
    "recursive_clustering_tree(dic_points,\n",
    "                          levels_clustering = 4,\n",
    "                          algorithm = 'dbscan',\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac000914-5378-408d-8485-3a46f0f05067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### recursive_clustering_tree\n",
       "\n",
       ">      recursive_clustering_tree (dic_points_ori, **kwargs)\n",
       "\n",
       "*Obtaing the recursive tree using a specific algorithm\n",
       "\n",
       ":param dict dic_points_ori: A dictionary with two keys 'points':['Array points'],\n",
       "                            'parent':'name_parent' \n",
       ":param int levels_clustering: levels to cluster  \n",
       ":returns TreeClusters: wih the clusters as nodes*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### recursive_clustering_tree\n",
       "\n",
       ">      recursive_clustering_tree (dic_points_ori, **kwargs)\n",
       "\n",
       "*Obtaing the recursive tree using a specific algorithm\n",
       "\n",
       ":param dict dic_points_ori: A dictionary with two keys 'points':['Array points'],\n",
       "                            'parent':'name_parent' \n",
       ":param int levels_clustering: levels to cluster  \n",
       ":returns TreeClusters: wih the clusters as nodes*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(recursive_clustering_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b244b2-2472-481e-97ca-ef84fde2b59e",
   "metadata": {},
   "source": [
    "## Similarity form metric\n",
    "\n",
    "The function use the *Jaccar* index and the points in the clusterization to obtain a metric that is used to compare the clusters on level basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1315c8-1cfb-41aa-ab33-f4db9bbab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def SSM(list_poly_c_1,list_poly_c_2 ,**kwargs):\n",
    "    \"\"\"\n",
    "    The function calculates the Similarity Shape Measurement (SSM)\n",
    "    between two clusterizations \n",
    "    \n",
    "    :param: list of nodes with points and polygons \n",
    "    \n",
    "    :param: list of nodes with points and polygons \n",
    "    \n",
    "    :param bool verbose: To print to debugg\n",
    "    \n",
    "    :returns double: The similarity mesuarment\n",
    "    \"\"\"\n",
    "    verbose= kwargs.get('verbose', False)\n",
    "    ##### Get intersection \n",
    "    list_de=[]\n",
    "    for i in list_poly_c_1:\n",
    "        list_de.append([ i.polygon_cluster.intersection(  j.polygon_cluster ) for  j in list_poly_c_2])\n",
    "    \n",
    "    list_de_bool = []\n",
    "    for i in list_de:\n",
    "        list_de_bool.append([not j.is_empty for j in i])\n",
    "    \n",
    "    list_de_index = []\n",
    "    #print(list_de_bool)\n",
    "    for i in list_de_bool:\n",
    "        if any(i):\n",
    "            list_de_index.append(i.index(True))\n",
    "        else:\n",
    "            list_de_index.append(None)\n",
    "    jacc_sim_po = []\n",
    "    for num, node in enumerate(list_poly_c_1):\n",
    "        ### ver eque pasa cuando se tienen 2 \n",
    "        if list_de_index[num] is not None:\n",
    "            node_get = list_poly_c_2[list_de_index[num]]\n",
    "            poli_int = list_de[num][list_de_index[num]]\n",
    "            \n",
    "            ####Puntos en la interseccion\n",
    "            #print(node)\n",
    "            points_all = node.get_point_decendent()\n",
    "            res_bool =[ poli_int.contains(p) for p in points_all] ### Como no necesito los puntos basta con esto\n",
    "            card = sum(res_bool)\n",
    "            ###Obtenemos jaccard \n",
    "            sim_jacc = (poli_int.area)/(node.polygon_cluster.area + node_get.polygon_cluster.area - poli_int.area)\n",
    "            if verbose:\n",
    "                print(\"jaccard: \" ,sim_jacc)\n",
    "                print(\"cardinal: \" ,card )\n",
    "            jacc_sim_po.append(sim_jacc* card)#####Cuando hay interseccion \n",
    "        else:\n",
    "            jacc_sim_po.append(0) #### Cuando no hay\n",
    "    \n",
    "    arr_bool = np.array(list_de_bool)\n",
    "\n",
    "    Q_not= []\n",
    "    for col in range(arr_bool.shape[1]):\n",
    "        cols_sel= arr_bool[:,col].any()\n",
    "        if cols_sel ==False:\n",
    "            Q_not.append(col)\n",
    "    #print(Q_not)\n",
    "    len_Q_not=[]\n",
    "    if Q_not:\n",
    "         len_Q_not =[len(list_poly_c_2[i].get_point_decendent())  for i in Q_not] \n",
    "\n",
    "    P_sum = sum([len(node.get_point_decendent())  for node in list_poly_c_1])\n",
    "    deno =P_sum + sum(len_Q_not)\n",
    "    return sum(jacc_sim_po)/deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46e0e5-a6a0-4216-9668-810b6321009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01897911518425685"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "##### Since the similarity shape measurment uses the polygons\n",
    "## to test it has to be on level basis\n",
    "\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "\n",
    "########## \n",
    "tree_DBSCAN = recursive_clustering_tree(dic_points,\n",
    "                          levels_clustering = 4,\n",
    "                          algorithm = 'dbscan',\n",
    "                         )\n",
    "\n",
    "SSM(HGC.levels_nodes[2], tree_DBSCAN.levels_nodes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24258920-1730-4e6e-8cd1-225f2e648f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### SSM\n",
       "\n",
       ">      SSM (list_poly_c_1, list_poly_c_2, **kwargs)\n",
       "\n",
       "*The function calculates the Similarity Shape Measurement (SSM)\n",
       "between two clusterizations \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param bool verbose: To print to debugg\n",
       "\n",
       ":returns double: The similarity mesuarment*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### SSM\n",
       "\n",
       ">      SSM (list_poly_c_1, list_poly_c_2, **kwargs)\n",
       "\n",
       "*The function calculates the Similarity Shape Measurement (SSM)\n",
       "between two clusterizations \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param bool verbose: To print to debugg\n",
       "\n",
       ":returns double: The similarity mesuarment*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0c4b7-1fee-4144-8325-b6b170ea4366",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These functions are use by the clustering algorithms and to create the structure, mosly by handly the names for each node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e2aab-1813-4071-b0df-f220810ea19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_alpha_shape\n",
       "\n",
       ">      get_alpha_shape (point_list)\n",
       "\n",
       "*Returns a polygon representing the hull of the points sample.\n",
       "\n",
       ":param list point_list: list list of tuples with samples coordinates.\n",
       "\n",
       ":returns shapely.Polygon: concave hull shapely polygon*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_alpha_shape\n",
       "\n",
       ">      get_alpha_shape (point_list)\n",
       "\n",
       "*Returns a polygon representing the hull of the points sample.\n",
       "\n",
       ":param list point_list: list list of tuples with samples coordinates.\n",
       "\n",
       ":returns shapely.Polygon: concave hull shapely polygon*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_alpha_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b1e75-e076-493b-8e00-9d57db72e4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### set_colinear\n",
       "\n",
       ">      set_colinear (list_points)\n",
       "\n",
       "*Check if in the list of points any of triplet of points\n",
       "is colinear\n",
       ":param list list_points: List of shapely Points\n",
       "\n",
       ":returns bool: True if all are not colinear*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### set_colinear\n",
       "\n",
       ">      set_colinear (list_points)\n",
       "\n",
       "*Check if in the list of points any of triplet of points\n",
       "is colinear\n",
       ":param list list_points: List of shapely Points\n",
       "\n",
       ":returns bool: True if all are not colinear*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(set_colinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55d0d9-9d04-40e1-8a3c-3a84d43df98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### collinear\n",
       "\n",
       ">      collinear (p1, p2, p3)\n",
       "\n",
       "*Check if the points are colinear \n",
       "\n",
       ":param shapely Point p1: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p2: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p3: point to chek if is colinear\n",
       "\n",
       ":return bool: True if are colinear*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### collinear\n",
       "\n",
       ">      collinear (p1, p2, p3)\n",
       "\n",
       "*Check if the points are colinear \n",
       "\n",
       ":param shapely Point p1: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p2: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p3: point to chek if is colinear\n",
       "\n",
       ":return bool: True if are colinear*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(collinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81270750-4c9e-4a99-94d2-6a64806b7232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_segments\n",
       "\n",
       ">      get_segments (points)\n",
       "\n",
       "*Get the segments from a delaunay triangulation\n",
       "\n",
       ":param points: Point to get Delaunay triangulation and exctract points \n",
       "\n",
       ":return edges:*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_segments\n",
       "\n",
       ">      get_segments (points)\n",
       "\n",
       "*Get the segments from a delaunay triangulation\n",
       "\n",
       ":param points: Point to get Delaunay triangulation and exctract points \n",
       "\n",
       ":return edges:*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc96d1-077d-4c02-b5b3-a745d1b4a096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_polygons_buf\n",
       "\n",
       ">      get_polygons_buf (lines)\n",
       "\n",
       "*Obtain the poligons from the lines\n",
       "\n",
       ":param list lines: List of lines\n",
       "\n",
       ":returns shapely polygon: the union of the union of \n",
       "edges (Polygon or multypolygon)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_polygons_buf\n",
       "\n",
       ">      get_polygons_buf (lines)\n",
       "\n",
       "*Obtain the poligons from the lines\n",
       "\n",
       ":param list lines: List of lines\n",
       "\n",
       ":returns shapely polygon: the union of the union of \n",
       "edges (Polygon or multypolygon)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_polygons_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde73025-f929-4f90-82a4-c62f83cd5459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### jaccard_distance\n",
       "\n",
       ">      jaccard_distance (p1, p2)\n",
       "\n",
       "*Computes the Jaccard similarity between two polygons.\n",
       "\n",
       "param: p1 shapely Poligon \n",
       "param: p2 shapely Poligon \n",
       "return float Jaccard distance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### jaccard_distance\n",
       "\n",
       ">      jaccard_distance (p1, p2)\n",
       "\n",
       "*Computes the Jaccard similarity between two polygons.\n",
       "\n",
       "param: p1 shapely Poligon \n",
       "param: p2 shapely Poligon \n",
       "return float Jaccard distance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(jaccard_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f91f8f-cad8-4121-b8cf-58ced4df2f9e",
   "metadata": {},
   "source": [
    "## Label functions \n",
    "The following functions are use to for labeling the points inside the clusters tree obtained using the cluastering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69b1da-2f25-4b0a-b749-d6783a12dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def labels_filtra(point_points, multy_pol):\n",
    "    \"\"\"\n",
    "    Labels the points in the multy_pol if no polygon contains \n",
    "    a point is label as -1\n",
    "    \n",
    "    :param shapely MultyPoint point_points: Points to check \n",
    "    \n",
    "    :param multy_pol\n",
    "    \n",
    "    :returns np.array: Label array with -1 if is not contained \n",
    "    in a polygon\n",
    "    \"\"\"\n",
    "    point_Po = [Point(i) for i in  point_points]\n",
    "    labels_p=[]\n",
    "    if type(multy_pol)==shapely.geometry.MultiPolygon :\n",
    "        for po in point_Po:\n",
    "            if multy_pol.contains(po):\n",
    "                for num_pol, poly in enumerate( multy_pol):\n",
    "                    if poly.contains(po):\n",
    "                        labels_p.append(num_pol)\n",
    "                        break\n",
    "            else:\n",
    "                labels_p.append(-1)\n",
    "    elif type(multy_pol)==shapely.geometry.Polygon :\n",
    "        for po in point_Po:\n",
    "            if multy_pol.contains(po):\n",
    "                labels_p.append(0)\n",
    "            else:\n",
    "                labels_p.append(-1)\n",
    "    else:\n",
    "        raise ValueError('The input is not MultiPolygon or Polygon type')   \n",
    "    \n",
    "    return np.array(labels_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d9a08-81ab-41ac-a65b-0a3524554e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  0, -1,  0, -1, -1, -1,  0, -1, -1, -1,  0, -1, -1, -1,  0,\n",
       "       -1,  0,  0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "x = [random.random() for i in range(20)] \n",
    "y = [random.random() for i in range(20)] \n",
    "poly = get_alpha_shape(points_check) \n",
    "labels_filtra (points_check, poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c8064-c68c-40b3-a790-5f48d87950ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### labels_filtra\n",
       "\n",
       ">      labels_filtra (point_points, multy_pol)\n",
       "\n",
       "*Labels the points in the multy_pol if no polygon contains \n",
       "a point is label as -1\n",
       "\n",
       ":param shapely MultyPoint point_points: Points to check \n",
       "\n",
       ":param multy_pol\n",
       "\n",
       ":returns np.array: Label array with -1 if is not contained \n",
       "in a polygon*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### labels_filtra\n",
       "\n",
       ">      labels_filtra (point_points, multy_pol)\n",
       "\n",
       "*Labels the points in the multy_pol if no polygon contains \n",
       "a point is label as -1\n",
       "\n",
       ":param shapely MultyPoint point_points: Points to check \n",
       "\n",
       ":param multy_pol\n",
       "\n",
       ":returns np.array: Label array with -1 if is not contained \n",
       "in a polygon*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(labels_filtra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff222b-a433-495c-a47c-e88fec4a9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def levels_from_strings(\n",
    "            string_tag,\n",
    "            level_str='l_',\n",
    "            node_str = 'n_',\n",
    "            **kwargs\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Returns the levels and the node id using the expected strings \n",
    "    that identify the level id and node id\n",
    "    \n",
    "    :param str level_str: string for the level\n",
    "    \n",
    "    :param str node_str: string for the nodes\n",
    "    \n",
    "    \n",
    "    :returns tuple (levels, nodeid): \n",
    "    \"\"\"\n",
    "    \n",
    "    positions = [i.start() for i in re.finditer( level_str, string_tag )]\n",
    "    nodeid_positions = [i.start() for i in re.finditer( node_str, string_tag )]    \n",
    "    \n",
    "    \n",
    "    #evels = [string_tag[i+len(level_str)] for i in positions ]\n",
    "    levels=[]\n",
    "    for i in positions:\n",
    "        if string_tag[i+len(level_str):].find(node_str) != -1:   \n",
    "            levels.append(string_tag[i+len(level_str):][:string_tag[i+len(level_str):].find(node_str)-1])\n",
    "        else:\n",
    "            levels.append(string_tag[i+len(level_str):])\n",
    "    nodeid=[]\n",
    "    for i in nodeid_positions:\n",
    "        if string_tag[i+len(node_str):].find(level_str) != -1:   \n",
    "            nodeid.append(string_tag[i+len(node_str):][:string_tag[i+len(node_str):].find(level_str)-1])\n",
    "        else:\n",
    "            nodeid.append(string_tag[i+len(node_str):])\n",
    "\n",
    "    return levels, nodeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59366f-6a24-4727-8f2c-24ee5f2cff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0', '1', '2'], ['0', '31', '2_'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "levels_from_strings('l_0_n_0_l_1_n_31_l_2_n_2_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70d440-641a-4b16-9bc5-fb3dd8931bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def level_tag(list_tags, level_int  ):\n",
    "    \"\"\"\n",
    "    Tags if the are noise or signal\n",
    "    \"\"\"\n",
    "    if len(list_tags)==0:\n",
    "        return 'noise'\n",
    "    try:\n",
    "        return list_tags[level_int]\n",
    "    except:\n",
    "        return 'noise'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252541be-b34e-4dbd-a87a-94e724ee1931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noise'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "dataframe_HGC = HGC.get_dataframe_recursive_node_label()\n",
    "\n",
    "level_tag(dataframe_HGC.iloc[-1]['cluster_id'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35825f-bb87-49fd-88e6-e61bc881c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def get_tag_level_df_labels(df, levels_int ):\n",
    "    \"\"\"\n",
    "    Get the tag for the cluster\n",
    "    \n",
    "    :param Pandas.DataFrame df:\n",
    "    \n",
    "    :param int levels_int: \n",
    "    \n",
    "    :returns None:\n",
    "    \"\"\"\n",
    "    for i in range(levels_int):\n",
    "        df['level_'+ str(i) +'_cluster']= df['cluster_id'].apply(lambda l:  level_tag(l,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07731f83-7666-4444-9daa-3d05a885fcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Points</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Final_tag</th>\n",
       "      <th>level</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>level_0_cluster</th>\n",
       "      <th>level_1_cluster</th>\n",
       "      <th>level_2_cluster</th>\n",
       "      <th>level_3_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>POINT (0.4319008587438852 0.3579592023717587)</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[0, 3, 1, 1_noise]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>POINT (0.4318974179489253 0.3579609951152309)</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[0, 3, 1, 1_noise]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>POINT (0.4318958733652622 0.3579631542280432)</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[0, 3, 1, 1_noise]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>POINT (0.4319039042807349 0.3579591487358343)</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[0, 3, 1, 1_noise]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>POINT (0.4319026080833185 0.3579632253482057)</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[0, 3, 1, 1_noise]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_noise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Points  \\\n",
       "915  POINT (0.4319008587438852 0.3579592023717587)   \n",
       "916  POINT (0.4318974179489253 0.3579609951152309)   \n",
       "917  POINT (0.4318958733652622 0.3579631542280432)   \n",
       "918  POINT (0.4319039042807349 0.3579591487358343)   \n",
       "919  POINT (0.4319026080833185 0.3579632253482057)   \n",
       "\n",
       "                                            Tag  \\\n",
       "915  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise   \n",
       "916  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise   \n",
       "917  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise   \n",
       "918  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise   \n",
       "919  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise   \n",
       "\n",
       "                                      Final_tag         level  \\\n",
       "915  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise  [0, 1, 2, 3]   \n",
       "916  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise  [0, 1, 2, 3]   \n",
       "917  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise  [0, 1, 2, 3]   \n",
       "918  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise  [0, 1, 2, 3]   \n",
       "919  Root_l_0_n_0_l_1_n_3_l_2_n_1_l_3_n_1_noise  [0, 1, 2, 3]   \n",
       "\n",
       "             cluster_id level_0_cluster level_1_cluster level_2_cluster  \\\n",
       "915  [0, 3, 1, 1_noise]               0               3               1   \n",
       "916  [0, 3, 1, 1_noise]               0               3               1   \n",
       "917  [0, 3, 1, 1_noise]               0               3               1   \n",
       "918  [0, 3, 1, 1_noise]               0               3               1   \n",
       "919  [0, 3, 1, 1_noise]               0               3               1   \n",
       "\n",
       "    level_3_cluster  \n",
       "915         1_noise  \n",
       "916         1_noise  \n",
       "917         1_noise  \n",
       "918         1_noise  \n",
       "919         1_noise  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "dataframe_HGC = HGC.get_dataframe_recursive_node_label(func_level_nodes = levels_from_strings)\n",
    "get_tag_level_df_labels(dataframe_HGC, 4)\n",
    "dataframe_HGC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a283af-e603-4c1d-8a10-8d4de49911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def get_mini_jaccars(cluster: NodeCluster, # A NodeCluster with a polygon to compare\n",
    "                     tree_2: TreeClusters, # A TreeClusters structure to compare  with the poligons to compare to\n",
    "                     level_int:int, #  The index of level of the polygons to compare\n",
    "                    )-> int :  # The index inside the level that is the most similar\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the most similar cluster in the tree_2 at level level_int\n",
    "\n",
    "    :param cluster: NodeCluster A NodeCluster with a polygon to compare\n",
    "\n",
    "    :param tree_2: TreeClusters A TreeClusters structure to compare  with the poligons to compare to\n",
    "\n",
    "    :param level_int:int The index of level of the polygons to compare\n",
    "    \n",
    "    returns int the index of the most similar polygon in the level\n",
    "    \"\"\"\n",
    "    tree_2_level= tree_2.get_level(level_int)\n",
    "    Jaccard_i= [jaccard_distance(cluster.polygon_cluster, j.polygon_cluster) for j in tree_2_level]  \n",
    "    valu_min = Jaccard_i.index( min(Jaccard_i))\n",
    "    return valu_min\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84868bae-1244-4a26-974a-9ce1f366c083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "tree_DBSCAN = recursive_clustering_tree(dic_points,\n",
    "                          levels_clustering = 4,\n",
    "                          algorithm = 'dbscan',\n",
    "                         )\n",
    "get_mini_jaccars(HGC.levels_nodes[2][0],tree_DBSCAN, 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465c9e2-2f07-491e-a679-c094f9118d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_mini_jaccars\n",
       "\n",
       ">      get_mini_jaccars\n",
       ">                        (cluster:HierarchicalGeoClustering.TreeClusters.NodeClu\n",
       ">                        ster, tree_2:HierarchicalGeoClustering.TreeClusters.Tre\n",
       ">                        eClusters, level_int:int)\n",
       "\n",
       "*Find the most similar cluster in the tree_2 at level level_int\n",
       "\n",
       ":param cluster: NodeCluster A NodeCluster with a polygon to compare\n",
       "\n",
       ":param tree_2: TreeClusters A TreeClusters structure to compare  with the poligons to compare to\n",
       "\n",
       ":param level_int:int The index of level of the polygons to compare\n",
       "\n",
       "returns int the index of the most similar polygon in the level*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| cluster | NodeCluster | A NodeCluster with a polygon to compare |\n",
       "| tree_2 | TreeClusters | A TreeClusters structure to compare  with the poligons to compare to |\n",
       "| level_int | int | The index of level of the polygons to compare |\n",
       "| **Returns** | **int** | **The index inside the level that is the most similar** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_mini_jaccars\n",
       "\n",
       ">      get_mini_jaccars\n",
       ">                        (cluster:HierarchicalGeoClustering.TreeClusters.NodeClu\n",
       ">                        ster, tree_2:HierarchicalGeoClustering.TreeClusters.Tre\n",
       ">                        eClusters, level_int:int)\n",
       "\n",
       "*Find the most similar cluster in the tree_2 at level level_int\n",
       "\n",
       ":param cluster: NodeCluster A NodeCluster with a polygon to compare\n",
       "\n",
       ":param tree_2: TreeClusters A TreeClusters structure to compare  with the poligons to compare to\n",
       "\n",
       ":param level_int:int The index of level of the polygons to compare\n",
       "\n",
       "returns int the index of the most similar polygon in the level*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| cluster | NodeCluster | A NodeCluster with a polygon to compare |\n",
       "| tree_2 | TreeClusters | A TreeClusters structure to compare  with the poligons to compare to |\n",
       "| level_int | int | The index of level of the polygons to compare |\n",
       "| **Returns** | **int** | **The index inside the level that is the most similar** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc( get_mini_jaccars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cfd27-6865-4b53-8cd1-e0e4372e6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def get_dics_labels(tree_or, tree_res, **kwargs):\n",
    "    \"\"\"\n",
    "    Obtains a list of dictionaries to retag the original tree_tag with their \n",
    "    correspondance in the tree_res on level level_get +1\n",
    "    \n",
    "    :param tree_or: Original tree\n",
    "    \n",
    "    :param tree_res: resulting tree\n",
    "    \n",
    "    :param level_get: int level to get\n",
    "    \n",
    "    :param return list of dictionaries:  \n",
    "    \"\"\"\n",
    "    dic_list_levels= []\n",
    "    ##### If the number of levels is different there is no reason that his should work \n",
    "    min_level_to_ask = min(tree_or.get_deepth(), tree_res.get_deepth())\n",
    "    \n",
    "    verbose= kwargs.get\n",
    "    for i in range(min_level_to_ask):\n",
    "        dic_level_df = get_label_clusters_df(tree_or, tree_res, i)\n",
    "        ## Eliminate the clusters with nan  \n",
    "        dic_level_df.dropna(axis=0, subset=['Sim_cluster'], inplace=True)\n",
    "        \n",
    "        dic_lev  = dic_level_df['Sim_cluster'].astype(int).to_dict()\n",
    "        dic_list_levels.append({'level_ori':'level_'+str(i)+'_cluster', 'dict': dic_lev})\n",
    "    return dic_list_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43c5ac-ca4c-48a4-9914-4133484cc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "def get_label_clusters_df(tree_1, tree_2, level_int):\n",
    "    \"\"\"\n",
    "    Obtains the dataframe with the label \n",
    "    \n",
    "    :param TreeClusters tree_1: \n",
    "    \n",
    "    :param TreeClusters tree_2:\n",
    "    \n",
    "    :param int level_int:\n",
    "    \n",
    "    :reutrns Pandas.DataFrame df_level_clus: \n",
    "    \"\"\"\n",
    "    level_all = tree_1.get_level(level_int)\n",
    "    df_level_clus = pd.DataFrame(level_all, columns=['Clusters'])\n",
    "    df_level_clus['Area'] = df_level_clus['Clusters'].apply(lambda l: l.polygon_cluster.area)\n",
    "    df_level_clus['Name'] = df_level_clus['Clusters'].apply(lambda l: l.name)\n",
    "    \n",
    "    df_level_clus['Sim_cluster'] = df_level_clus['Clusters'].apply(lambda l: get_mini_jaccars(l, tree_2,level_int)) ###### Como se hacen las clusterizaciones se debe usar el siguiente nivel\n",
    "    \n",
    "    #print('', df_level_clus['Sim_cluster'].dtype)\n",
    "    df_level_clus= df_level_clus.sort_values(by ='Area', ascending=False)\n",
    "    df_level_clus['Sim_cluster'] = (df_level_clus['Sim_cluster']\n",
    "                                    .where(~df_level_clus.duplicated(subset=['Sim_cluster']), None))\n",
    "    #print(df_level_clus['Sim_cluster'].dtype)\n",
    "    level_2= tree_2.get_level(level_int) \n",
    "    df_level_clus['Sim_cluster_name'] =(df_level_clus['Sim_cluster']\n",
    "                                         .astype('int32', errors='ignore')\n",
    "                                         .replace({np.nan: ''})\n",
    "                                         .apply(lambda l:  level_2[int(l)].name if l !=''  else None) )\n",
    "    \n",
    "    return df_level_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f2a5e-a87c-4a9b-b9d3-3c583944479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level_ori': 'level_0_cluster', 'dict': {0: 0}},\n",
       " {'level_ori': 'level_1_cluster', 'dict': {0: 0.0}},\n",
       " {'level_ori': 'level_2_cluster', 'dict': {1: 0.0, 0: 1.0}},\n",
       " {'level_ori': 'level_3_cluster', 'dict': {0: 1.0, 1: 2.0, 4: 0.0}},\n",
       " {'level_ori': 'level_4_cluster', 'dict': {0: 0.0, 2: 2.0, 4: 3.0, 9: 1.0}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "####It is use to get the labels differenc between the original tree and\n",
    "## the tree\n",
    "\n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "tree_DBSCAN = recursive_clustering_tree(dic_points,\n",
    "                          levels_clustering = 4,\n",
    "                          algorithm = 'dbscan',\n",
    "                         )\n",
    "\n",
    "get_dics_labels(HGC, tree_DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cd295-beef-4310-8b23-c20c43e6ae12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_dics_labels\n",
       "\n",
       ">      get_dics_labels (tree_or, tree_res, **kwargs)\n",
       "\n",
       "*Obtains a list of dictionaries to retag the original tree_tag with their \n",
       "correspondance in the tree_res on level level_get +1\n",
       "\n",
       ":param tree_or: Original tree\n",
       "\n",
       ":param tree_res: resulting tree\n",
       "\n",
       ":param level_get: int level to get\n",
       "\n",
       ":param return list of dictionaries:*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_dics_labels\n",
       "\n",
       ">      get_dics_labels (tree_or, tree_res, **kwargs)\n",
       "\n",
       "*Obtains a list of dictionaries to retag the original tree_tag with their \n",
       "correspondance in the tree_res on level level_get +1\n",
       "\n",
       ":param tree_or: Original tree\n",
       "\n",
       ":param tree_res: resulting tree\n",
       "\n",
       ":param level_get: int level to get\n",
       "\n",
       ":param return list of dictionaries:*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_dics_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ade0b6-6b4f-4cce-ba3f-bf82e95186dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export \n",
    "def mod_cid_label(dic_label:dict # Dictionary from 'get_dic_labels' function\n",
    "                 )-> dict: # Dictionary with the 'level_ori', 'dict' and 'noise' keys\n",
    "    \"\"\"\n",
    "    Get a dictionary for the labels that will be used from \n",
    "    the dictionaries obtain by  the 'get_dic_labels' function.\n",
    "    The 'dict' dictionary  conntains the tag that the clusters should be use. \n",
    "    \"\"\"\n",
    "    dic_label={str(k):str(v) for k,v in dic_label.items()} \n",
    "    dic_label['noise'] = 'noise'\n",
    "    return dic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f995b74-be29-458c-8bbc-2ee7260694e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level_ori': 'level_3_cluster',\n",
       " 'dict': '{0: 0.0, 1: 1.0, 3: 2.0, 4: 3.0, 5: 4.0}',\n",
       " 'noise': 'noise'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "HGC = TreeClusters(4, random_seed= 124)\n",
    "HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "original_points= HGC.get_points_tree()\n",
    "X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "tree_DBSCAN = recursive_clustering_tree(dic_points,\n",
    "                          levels_clustering = 4,\n",
    "                          algorithm = 'dbscan',\n",
    "                         )\n",
    "\n",
    "dics_DBSCAN = get_dics_labels(HGC, tree_DBSCAN)\n",
    "mod_cid_label(dics_DBSCAN[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdb71a-bce8-42f2-babb-c328c1b6c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### mod_cid_label\n",
       "\n",
       ">      mod_cid_label (dic_label:dict)\n",
       "\n",
       "*Get a dictionary for the labels that will be used from \n",
       "the dictionaries obtain by  the 'get_dic_labels' function.\n",
       "The 'dict' dictionary  conntains the tag that the clusters should be use.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dic_label | dict | Dictionary from 'get_dic_labels' function |\n",
       "| **Returns** | **dict** | **Dictionary with the 'level_ori', 'dict' and 'noise' keys** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### mod_cid_label\n",
       "\n",
       ">      mod_cid_label (dic_label:dict)\n",
       "\n",
       "*Get a dictionary for the labels that will be used from \n",
       "the dictionaries obtain by  the 'get_dic_labels' function.\n",
       "The 'dict' dictionary  conntains the tag that the clusters should be use.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dic_label | dict | Dictionary from 'get_dic_labels' function |\n",
       "| **Returns** | **dict** | **Dictionary with the 'level_ori', 'dict' and 'noise' keys** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(mod_cid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aeeec4-9718-42f8-8261-c112c090c06f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2481884834.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[82], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_fram_or['re_tag_'+str(df_results.name)+'_'+tag_original] =\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def retag_originals(df_fram_or: pd.DataFrame ,\n",
    "                    df_results: pd.DataFrame,\n",
    "                    tag_original: str,\n",
    "                    tag_results:str,\n",
    "                    dic_tag_or_res:dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Retags the labels in the df_fram_or using the dictionary dic_tag_or_res\n",
    "    to match the tags with the corresponding tag in the df_result \n",
    "    and all the labels that are not in the dictionary generate a \n",
    "    new tag fo them. \n",
    "    \n",
    "    :param Pandas.DataFrame df_fram_or\n",
    "    \n",
    "    :param Pandas.DataFrame df_results\n",
    "    \n",
    "    :param tag_original\n",
    "    \n",
    "    :param tag_results\n",
    "    \n",
    "    :param Pandas.DataFrame dic_tag_or_res \n",
    "    \n",
    "    \"\"\"\n",
    "    tag_plus=  len(df_results[tag_results].unique()) +100  - len(df_results[tag_results].unique())%100\n",
    "    df_fram_or['re_tag_'+str(df_results.name)+'_'+tag_original] = df_fram_or[tag_original].apply(lambda l: dic_tag_or_res[l] if l in dic_tag_or_res.keys() else   str(int(l) +tag_plus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fae32-5972-4250-8116-6dd1c540eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "\n",
    "# HGC = TreeClusters(4, random_seed= 124)\n",
    "# HGC.populate_tree(number_per_cluster=40, avoid_intersec= True)\n",
    "# original_points= HGC.get_points_tree()\n",
    "# X_2=np.array([[p.x,p.y] for p in original_points])\n",
    "# dic_points={'points':[X_2], 'parent':''}\n",
    "\n",
    "# tree_DBSCAN_t = recursive_clustering_tree(dic_points,\n",
    "#                           levels_clustering = 4,\n",
    "#                           algorithm = 'dbscan',\n",
    "#                          )\n",
    "\n",
    "# data_fram_HGC = HGC.get_dataframe_recursive_node_label(\n",
    "#                     func_level_nodes = levels_from_strings\n",
    "#                 )\n",
    "# data_fram_DBSCAN = tree_DBSCAN_t.get_dataframe_recursive_node_label()\n",
    "\n",
    "# get_tag_level_df_labels(data_fram_HGC, 4)\n",
    "# dic_levels_DBSCAN = get_dics_labels(HGC, tree_DBSCAN_t)\n",
    "# get_tag_level_df_labels(data_fram_DBSCAN, 4)\n",
    "# dic_label_levels_DBSCAN=[ {'level_ori':dic['level_ori'],\n",
    "#                                  'dict':mod_cid_label(dic['dict']) } \n",
    "#                                    for dic in  dic_levels_DBSCAN\n",
    "#                               ]\n",
    "\n",
    "# for dic in dic_label_levels_DBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "#     tag_ori = dic['level_ori']\n",
    "#     dic_lev = dic['dict']\n",
    "#     retag_originals(data_fram_HGC,\n",
    "#                     data_fram_DBSCAN,\n",
    "#                     tag_ori,\n",
    "#                     tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "#                     dic_lev)\n",
    "# # retag_originals(data_fram_HGC,\n",
    "# #                 data_fram__DBSCAN,\n",
    "# #                 dic_label_levels_DBSCAN[3]['level_ori'],\n",
    "# #                 dic_label_levels_DBSCAN[3]['level_ori'],\n",
    "# #                 dic_label_levels_DBSCAN[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55727dde-cd55-4baa-ac74-60782bcc3a1e",
   "metadata": {},
   "source": [
    "## All as a single function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8da47d-30f6-4526-a36c-310b2d09f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "def generate_tree_clusterize_form(**kwargs ):\n",
    "    \"\"\"\n",
    "    Generates all the experiment all the experiment creates the data and clusterize using the algorithm available\n",
    "    \n",
    "    :param levels_tree: Levels for the tree\n",
    "    \n",
    "    :param int per_cluster: Points per clusters\n",
    "    \n",
    "    :param levels_cluster:  Levels to clusterize\n",
    "    \n",
    "    :param bool verbose:  To print some outputs \n",
    "    \n",
    "    :returns: a dictionary with all the  data frames and a dictionary \n",
    "    with the similarity measurment created\n",
    "     \"\"\"\n",
    "    \n",
    "    levels_tree= kwargs.get('tree_level', 4)\n",
    "    per_cluster = kwargs.get('num_per_cluster', 200)\n",
    "    levels_cluster = kwargs.get('levels_cluster', 4)\n",
    "    verbose = kwargs.get('verbose', False)\n",
    "    \n",
    "    if verbose:\n",
    "        print('generating tree')\n",
    "    \n",
    "    \n",
    "    random.seed(int(time.time()))\n",
    "    random_seed = random.randint(0,1500)\n",
    "    print('Random to use: ',random_seed )\n",
    "    print('With',levels_tree , ' levels' )\n",
    "    tree_original= TreeClusters(levels_tree, random_seed= random_seed)\n",
    "    tree_original.populate_tree(number_per_cluster=per_cluster, **kwargs)\n",
    "    tree_original_points= tree_original.get_points_tree()\n",
    "    X_2=np.array([[p.x,p.y] for p in tree_original_points])\n",
    "    dic_points_ori={'points':[X_2], 'parent':''}\n",
    "    if verbose:\n",
    "        print('tree with: ', X_2.shape )\n",
    "    \n",
    "    while X_2.shape[0] < 2000:\n",
    "        \n",
    "        print('tree with too few elements to clusterize creating new tree')\n",
    "        random.seed(int(time.time()))\n",
    "        random_seed = random.randint(0,1500)\n",
    "        print('Random to use: ',random_seed )\n",
    "        tree_original= TreeClusters(levels_tree, random_seed= random_seed)\n",
    "        tree_original.populate_tree(number_per_cluster=per_cluster,\n",
    "                                    avoid_intersec= True)\n",
    "        tree_original_points= tree_original.get_points_tree()\n",
    "        X_2=np.array([[p.x,p.y] for p in tree_original_points])\n",
    "        dic_points_ori={'points':[X_2], 'parent':''}\n",
    "        \n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('tree generated')\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('clusterize and creating the trees')\n",
    "    \n",
    "    tree_Natural_c = recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'natural_cities', \n",
    "                                               **kwargs,\n",
    "                                              )\n",
    "    tree_DBSCAN = recursive_clustering_tree(dic_points_ori,\n",
    "                                              levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'dbscan',\n",
    "                                              **kwargs,\n",
    "                                           )\n",
    "    tree_HDBSCAN = recursive_clustering_tree(dic_points_ori,\n",
    "                                              levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'hdbscan',\n",
    "                                              **kwargs,\n",
    "                                            )\n",
    "    tree_OPTICS= recursive_clustering_tree(dic_points_ori,\n",
    "                                              levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'optics',\n",
    "                                              **kwargs,\n",
    "                                            )\n",
    "    tree_Adap_DBSCAN = recursive_clustering_tree(dic_points_ori,\n",
    "                                              levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'adaptative_DBSCAN',\n",
    "                                              **kwargs,\n",
    "                                            )\n",
    "    if verbose:\n",
    "        print('DONE clusterize and creating the trees')\n",
    "    ######  get the points dataframe for each tree \n",
    "    data_fram_or = tree_original.get_dataframe_recursive_node_label(func_level_nodes = levels_from_strings)\n",
    "    df_Natural = tree_Natural_c.get_dataframe_recursive_node_label()\n",
    "    df_DBSCAN = tree_DBSCAN.get_dataframe_recursive_node_label()\n",
    "    df_HDBSCAN = tree_HDBSCAN.get_dataframe_recursive_node_label()\n",
    "    df_OPTICS = tree_OPTICS.get_dataframe_recursive_node_label()\n",
    "    df_Adap_DBSCAN = tree_Adap_DBSCAN.get_dataframe_recursive_node_label()\n",
    "    \n",
    "    df_Natural.name='Natural_C'\n",
    "    df_DBSCAN.name= 'DBSCAN'\n",
    "    df_HDBSCAN.name= 'HDBSCAN'\n",
    "    df_OPTICS.name= 'OPTICS'\n",
    "    df_Adap_DBSCAN.name = 'Adap_DBSCAN'\n",
    "    \n",
    "    if verbose:\n",
    "        print('Original size',data_fram_or.shape )\n",
    "        print('Natural size',df_Natural.shape)\n",
    "        print('DBSCAN size',df_DBSCAN.shape)\n",
    "        print('HDBSCAN size',df_HDBSCAN.shape)\n",
    "        print('OPTICS size',df_OPTICS.shape)\n",
    "        print('adaptative_DBSCAN size',df_Adap_DBSCAN.shape)\n",
    "    \n",
    "    ######For each dataframe  \n",
    "    if verbose:\n",
    "        print('get dataframe Original')\n",
    "    get_tag_level_df_labels(data_fram_or, levels_cluster)\n",
    "    ###Natural Cities\n",
    "    if verbose:\n",
    "        print('get dataframe Natural cities')\n",
    "        \n",
    "    dic_final_levels_Natural_c = get_dics_labels(tree_original, tree_Natural_c)\n",
    "    dic_label_final_levels_Natural=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_Natural_c]\n",
    "    get_tag_level_df_labels(df_Natural, levels_cluster)\n",
    "    for dic in dic_label_final_levels_Natural[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_Natural,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    \n",
    "    \n",
    "    ### DBSCAN\n",
    "    if verbose:\n",
    "        print('get dataframe DBSCAN')\n",
    "        \n",
    "    dic_final_levels_DBSCAN = get_dics_labels(tree_original, tree_DBSCAN)\n",
    "    dic_label_final_levels_DBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_DBSCAN]\n",
    "    get_tag_level_df_labels(df_DBSCAN, levels_cluster)\n",
    "    for dic in dic_label_final_levels_DBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_DBSCAN,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    \n",
    "    \n",
    "    ##HDBSCAN\n",
    "    if verbose:\n",
    "        print('get dataframe HDBSCAN')\n",
    "    dic_final_levels_HDBSCAN = get_dics_labels(tree_original, tree_HDBSCAN)\n",
    "    dic_label_final_levels_HDBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_HDBSCAN]\n",
    "    get_tag_level_df_labels(df_HDBSCAN, levels_cluster)\n",
    "    for dic in dic_label_final_levels_HDBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_HDBSCAN,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    #### OPTICS\n",
    "    if verbose:\n",
    "        print('get dataframe OPTICS')\n",
    "        \n",
    "    dic_final_levels_OPTICS = get_dics_labels(tree_original, tree_OPTICS)\n",
    "    dic_label_final_levels_OPTICS=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_OPTICS]\n",
    "    get_tag_level_df_labels(df_OPTICS, levels_cluster)\n",
    "    for dic in dic_label_final_levels_OPTICS[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_OPTICS,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    ##### Adap_DBSCAN\n",
    "    if verbose:\n",
    "        print('get adaptative DBSCAN')\n",
    "        \n",
    "    dic_final_levels_Adap_DBSCAN = get_dics_labels(tree_original, tree_Adap_DBSCAN)\n",
    "    dic_label_final_levels_Adap_DBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_Adap_DBSCAN]\n",
    "    get_tag_level_df_labels(df_Adap_DBSCAN, levels_cluster)\n",
    "    for dic in dic_label_final_levels_Adap_DBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_Adap_DBSCAN,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    ##############################################Get only signal  noise \n",
    "    data_fram_or_sig_noise = tree_original.get_tag_noise_signal_tree()\n",
    "    df_Natural_sig_noise = tree_Natural_c.get_tag_noise_signal_tree()\n",
    "    df_DBSCAN_sig_noise = tree_DBSCAN.get_tag_noise_signal_tree()\n",
    "    df_HDBSCAN_sig_noise = tree_HDBSCAN.get_tag_noise_signal_tree()\n",
    "    df_OPTICS_sig_noise = tree_OPTICS.get_tag_noise_signal_tree()\n",
    "    df_Adap_DBSCAN_sig_noise = tree_Adap_DBSCAN.get_tag_noise_signal_tree()\n",
    "    \n",
    "    df_Natural_sig_noise.name='I_Natural_C'\n",
    "    df_DBSCAN_sig_noise.name= 'I_DBSCAN'\n",
    "    df_HDBSCAN_sig_noise.name= 'I_HDBSCAN'\n",
    "    df_OPTICS_sig_noise.name= 'I_OPTICS'\n",
    "    df_Adap_DBSCAN_sig_noise.name = 'I_Adap_DBSCAN'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######Evaluate form metric#| hide \n",
    "### Test\n",
    "    if verbose:\n",
    "        print('Niveles: ' ,len(tree_original.levels_nodes))\n",
    "        print('Nodos en el ultimo nivel: ' ,len(tree_original.levels_nodes[-1]))\n",
    "        #print('tag_all : ' , data_fram_or['Final_tag'].unique())\n",
    "    levels_r = range(0, levels_cluster)###\n",
    "    resultado_form_metric = []\n",
    "    for l in levels_r:\n",
    "        \n",
    "        d = { 'Level': l,\n",
    "            'DBSCAN': SSM(tree_original.levels_nodes[l],\n",
    "                                            tree_DBSCAN.levels_nodes[l]),\n",
    "             'HDBSCAN': SSM(tree_original.levels_nodes[l],\n",
    "                                             tree_HDBSCAN.levels_nodes[l]),\n",
    "             'Natural': SSM(tree_original.levels_nodes[l],\n",
    "                                             tree_Natural_c.levels_nodes[l]),\n",
    "             'OPTICS': SSM(tree_original.levels_nodes[l],\n",
    "                                            tree_OPTICS.levels_nodes[l]),\n",
    "             'Adap_DBSCAN': SSM(tree_original.levels_nodes[l],\n",
    "                                          tree_Adap_DBSCAN.levels_nodes[l])\n",
    "             }\n",
    "        resultado_form_metric.append(d)\n",
    "    \n",
    "\n",
    "    return {'Point_dataframes':{'original_retag':data_fram_or,\n",
    "            'Natural_C':df_Natural ,\n",
    "            'DBSCAN':df_DBSCAN,\n",
    "            'HDBSCAN':df_HDBSCAN,\n",
    "            'OPTICS':df_OPTICS,\n",
    "            'Adap_DBSCAN':df_Adap_DBSCAN\n",
    "           }, 'metric_form': resultado_form_metric,\n",
    "            'Noise_signal':{\n",
    "            'original_retag':data_fram_or_sig_noise,\n",
    "            'Natural_C':df_Natural_sig_noise ,\n",
    "            'DBSCAN':df_DBSCAN_sig_noise,\n",
    "            'HDBSCAN':df_HDBSCAN_sig_noise,\n",
    "            'OPTICS':df_OPTICS_sig_noise,\n",
    "            'Adap_DBSCAN':df_Adap_DBSCAN_sig_noise\n",
    "            }\n",
    "           \n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6523e-a8c5-4f18-b4d2-53f50ce539e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "##### Generate all the clustering hierarchical structures and put it on trees\n",
    "\n",
    "# generate_tree_clusterize_form(tree_level= 3,\n",
    "                            #   per_cluster = 50,\n",
    "                            #   levels_cluster =  3,\n",
    "                            #   verbose = True,\n",
    "                            #   return_noise= True,\n",
    "                            #  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e708eb-d16b-4ad2-b21c-459ca8151248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### generate_tree_clusterize_form\n",
       "\n",
       ">      generate_tree_clusterize_form (**kwargs)\n",
       "\n",
       "*Generates all the experiment all the experiment creates the data and clusterize using the algorithm available\n",
       "\n",
       ":param levels_tree: Levels for the tree\n",
       "\n",
       ":param int per_cluster: Points per clusters\n",
       "\n",
       ":param levels_cluster:  Levels to clusterize\n",
       "\n",
       ":param bool verbose:  To print some outputs \n",
       "\n",
       ":returns: a dictionary with all the  data frames and a dictionary \n",
       "with the similarity measurment created*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### generate_tree_clusterize_form\n",
       "\n",
       ">      generate_tree_clusterize_form (**kwargs)\n",
       "\n",
       "*Generates all the experiment all the experiment creates the data and clusterize using the algorithm available\n",
       "\n",
       ":param levels_tree: Levels for the tree\n",
       "\n",
       ":param int per_cluster: Points per clusters\n",
       "\n",
       ":param levels_cluster:  Levels to clusterize\n",
       "\n",
       ":param bool verbose:  To print some outputs \n",
       "\n",
       ":returns: a dictionary with all the  data frames and a dictionary \n",
       "with the similarity measurment created*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(generate_tree_clusterize_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24e260-03ec-4062-a606-77c313efba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "###Test\n",
    "# generate_tree_clusterize_form(tree_level= 3,\n",
    "#                               per_cluster = 50,\n",
    "#                               levels_cluster =  3,\n",
    "#                               # verbose = True,\n",
    "#                               return_noise= True,\n",
    "                              \n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f3342-26f3-474b-b899-3b47286edf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
